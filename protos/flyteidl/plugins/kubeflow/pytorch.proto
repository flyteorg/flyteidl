syntax = "proto3";

package flyteidl.plugins.kubeflow;

option go_package = "github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/plugins";

import "flyteidl/core/tasks.proto";
import "flyteidl/plugins/kubeflow/common.proto";

// Proto for plugin that enables distributed training using https://github.com/kubeflow/pytorch-operator
message DistributedPyTorchTrainingTask {
  // Worker replicas spec
  DistributedPyTorchTrainingReplicaSpec worker_replicas = 1;

  // Master replicas spec, master replicas can only have 1 replica
  DistributedPyTorchTrainingReplicaSpec master_replicas = 2;

  // RunPolicy encapsulates various runtime policies of the distributed training
  // job, for example how to clean up resources and how long the job can stay
  // active.
  RunPolicy run_policy = 3;
}

message DistributedPyTorchTrainingReplicaSpec {
  // Number of replicas
  int32 replicas = 1;

  // Image used for the replica group
  string image = 2;

  // Resources required for the replica group
  core.Resources resources = 3;
  
  // RestartPolicy determines whether pods will be restarted when they exit
  RestartPolicy restart_policy = 4;
}
