// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: flyteidl/plugins/spark.proto

#include "flyteidl/plugins/spark.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

extern PROTOBUF_INTERNAL_EXPORT_flyteidl_2fplugins_2fspark_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_SparkJob_HadoopConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_flyteidl_2fplugins_2fspark_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_SparkJob_SparkConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_google_2fprotobuf_2fstruct_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_ListValue_google_2fprotobuf_2fstruct_2eproto;
namespace flyteidl {
namespace plugins {
class SparkApplicationDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<SparkApplication> _instance;
} _SparkApplication_default_instance_;
class SparkJob_SparkConfEntry_DoNotUseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<SparkJob_SparkConfEntry_DoNotUse> _instance;
} _SparkJob_SparkConfEntry_DoNotUse_default_instance_;
class SparkJob_HadoopConfEntry_DoNotUseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<SparkJob_HadoopConfEntry_DoNotUse> _instance;
} _SparkJob_HadoopConfEntry_DoNotUse_default_instance_;
class SparkJobDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<SparkJob> _instance;
} _SparkJob_default_instance_;
}  // namespace plugins
}  // namespace flyteidl
static void InitDefaultsSparkApplication_flyteidl_2fplugins_2fspark_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::flyteidl::plugins::_SparkApplication_default_instance_;
    new (ptr) ::flyteidl::plugins::SparkApplication();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::flyteidl::plugins::SparkApplication::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_SparkApplication_flyteidl_2fplugins_2fspark_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsSparkApplication_flyteidl_2fplugins_2fspark_2eproto}, {}};

static void InitDefaultsSparkJob_SparkConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::flyteidl::plugins::_SparkJob_SparkConfEntry_DoNotUse_default_instance_;
    new (ptr) ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse();
  }
  ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_SparkJob_SparkConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsSparkJob_SparkConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto}, {}};

static void InitDefaultsSparkJob_HadoopConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::flyteidl::plugins::_SparkJob_HadoopConfEntry_DoNotUse_default_instance_;
    new (ptr) ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse();
  }
  ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_SparkJob_HadoopConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsSparkJob_HadoopConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto}, {}};

static void InitDefaultsSparkJob_flyteidl_2fplugins_2fspark_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::flyteidl::plugins::_SparkJob_default_instance_;
    new (ptr) ::flyteidl::plugins::SparkJob();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::flyteidl::plugins::SparkJob::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<3> scc_info_SparkJob_flyteidl_2fplugins_2fspark_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 3, InitDefaultsSparkJob_flyteidl_2fplugins_2fspark_2eproto}, {
      &scc_info_SparkJob_SparkConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto.base,
      &scc_info_SparkJob_HadoopConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto.base,
      &scc_info_ListValue_google_2fprotobuf_2fstruct_2eproto.base,}};

void InitDefaults_flyteidl_2fplugins_2fspark_2eproto() {
  ::google::protobuf::internal::InitSCC(&scc_info_SparkApplication_flyteidl_2fplugins_2fspark_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_SparkJob_SparkConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_SparkJob_HadoopConfEntry_DoNotUse_flyteidl_2fplugins_2fspark_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_SparkJob_flyteidl_2fplugins_2fspark_2eproto.base);
}

::google::protobuf::Metadata file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[4];
const ::google::protobuf::EnumDescriptor* file_level_enum_descriptors_flyteidl_2fplugins_2fspark_2eproto[1];
constexpr ::google::protobuf::ServiceDescriptor const** file_level_service_descriptors_flyteidl_2fplugins_2fspark_2eproto = nullptr;

const ::google::protobuf::uint32 TableStruct_flyteidl_2fplugins_2fspark_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkApplication, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, value_),
  0,
  1,
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, applicationtype_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, mainapplicationfile_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, mainclass_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, sparkconf_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, hadoopconf_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, executorpath_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, databricksconf_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, databrickstoken_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, databricksinstance_),
};
static const ::google::protobuf::internal::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, sizeof(::flyteidl::plugins::SparkApplication)},
  { 5, 12, sizeof(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse)},
  { 14, 21, sizeof(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse)},
  { 23, -1, sizeof(::flyteidl::plugins::SparkJob)},
};

static ::google::protobuf::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::google::protobuf::Message*>(&::flyteidl::plugins::_SparkApplication_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::flyteidl::plugins::_SparkJob_SparkConfEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::flyteidl::plugins::_SparkJob_HadoopConfEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::flyteidl::plugins::_SparkJob_default_instance_),
};

::google::protobuf::internal::AssignDescriptorsTable assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto = {
  {}, AddDescriptors_flyteidl_2fplugins_2fspark_2eproto, "flyteidl/plugins/spark.proto", schemas,
  file_default_instances, TableStruct_flyteidl_2fplugins_2fspark_2eproto::offsets,
  file_level_metadata_flyteidl_2fplugins_2fspark_2eproto, 4, file_level_enum_descriptors_flyteidl_2fplugins_2fspark_2eproto, file_level_service_descriptors_flyteidl_2fplugins_2fspark_2eproto,
};

const char descriptor_table_protodef_flyteidl_2fplugins_2fspark_2eproto[] =
  "\n\034flyteidl/plugins/spark.proto\022\020flyteidl"
  ".plugins\032\034google/protobuf/struct.proto\"B"
  "\n\020SparkApplication\".\n\004Type\022\n\n\006PYTHON\020\000\022\010"
  "\n\004JAVA\020\001\022\t\n\005SCALA\020\002\022\005\n\001R\020\003\"\333\003\n\010SparkJob\022"
  "@\n\017applicationType\030\001 \001(\0162\'.flyteidl.plug"
  "ins.SparkApplication.Type\022\033\n\023mainApplica"
  "tionFile\030\002 \001(\t\022\021\n\tmainClass\030\003 \001(\t\022<\n\tspa"
  "rkConf\030\004 \003(\0132).flyteidl.plugins.SparkJob"
  ".SparkConfEntry\022>\n\nhadoopConf\030\005 \003(\0132*.fl"
  "yteidl.plugins.SparkJob.HadoopConfEntry\022"
  "\024\n\014executorPath\030\006 \001(\t\022/\n\016databricksConf\030"
  "\007 \001(\0132\027.google.protobuf.Struct\022\027\n\017databr"
  "icksToken\030\010 \001(\t\022\032\n\022databricksInstance\030\t "
  "\001(\t\0320\n\016SparkConfEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005va"
  "lue\030\002 \001(\t:\0028\001\0321\n\017HadoopConfEntry\022\013\n\003key\030"
  "\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001B9Z7github.com/f"
  "lyteorg/flyteidl/gen/pb-go/flyteidl/plug"
  "insb\006proto3"
  ;
::google::protobuf::internal::DescriptorTable descriptor_table_flyteidl_2fplugins_2fspark_2eproto = {
  false, InitDefaults_flyteidl_2fplugins_2fspark_2eproto, 
  descriptor_table_protodef_flyteidl_2fplugins_2fspark_2eproto,
  "flyteidl/plugins/spark.proto", &assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto, 691,
};

void AddDescriptors_flyteidl_2fplugins_2fspark_2eproto() {
  static constexpr ::google::protobuf::internal::InitFunc deps[1] =
  {
    ::AddDescriptors_google_2fprotobuf_2fstruct_2eproto,
  };
 ::google::protobuf::internal::AddDescriptors(&descriptor_table_flyteidl_2fplugins_2fspark_2eproto, deps, 1);
}

// Force running AddDescriptors() at dynamic initialization time.
static bool dynamic_init_dummy_flyteidl_2fplugins_2fspark_2eproto = []() { AddDescriptors_flyteidl_2fplugins_2fspark_2eproto(); return true; }();
namespace flyteidl {
namespace plugins {
const ::google::protobuf::EnumDescriptor* SparkApplication_Type_descriptor() {
  ::google::protobuf::internal::AssignDescriptors(&assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto);
  return file_level_enum_descriptors_flyteidl_2fplugins_2fspark_2eproto[0];
}
bool SparkApplication_Type_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const SparkApplication_Type SparkApplication::PYTHON;
const SparkApplication_Type SparkApplication::JAVA;
const SparkApplication_Type SparkApplication::SCALA;
const SparkApplication_Type SparkApplication::R;
const SparkApplication_Type SparkApplication::Type_MIN;
const SparkApplication_Type SparkApplication::Type_MAX;
const int SparkApplication::Type_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

// ===================================================================

void SparkApplication::InitAsDefaultInstance() {
}
class SparkApplication::HasBitSetters {
 public:
};

#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SparkApplication::SparkApplication()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:flyteidl.plugins.SparkApplication)
}
SparkApplication::SparkApplication(const SparkApplication& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:flyteidl.plugins.SparkApplication)
}

void SparkApplication::SharedCtor() {
}

SparkApplication::~SparkApplication() {
  // @@protoc_insertion_point(destructor:flyteidl.plugins.SparkApplication)
  SharedDtor();
}

void SparkApplication::SharedDtor() {
}

void SparkApplication::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const SparkApplication& SparkApplication::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_SparkApplication_flyteidl_2fplugins_2fspark_2eproto.base);
  return *internal_default_instance();
}


void SparkApplication::Clear() {
// @@protoc_insertion_point(message_clear_start:flyteidl.plugins.SparkApplication)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* SparkApplication::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<SparkApplication*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      default: {
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool SparkApplication::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:flyteidl.plugins.SparkApplication)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormat::SkipField(
          input, tag, _internal_metadata_.mutable_unknown_fields()));
  }
success:
  // @@protoc_insertion_point(parse_success:flyteidl.plugins.SparkApplication)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:flyteidl.plugins.SparkApplication)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void SparkApplication::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:flyteidl.plugins.SparkApplication)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:flyteidl.plugins.SparkApplication)
}

::google::protobuf::uint8* SparkApplication::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:flyteidl.plugins.SparkApplication)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:flyteidl.plugins.SparkApplication)
  return target;
}

size_t SparkApplication::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:flyteidl.plugins.SparkApplication)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void SparkApplication::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:flyteidl.plugins.SparkApplication)
  GOOGLE_DCHECK_NE(&from, this);
  const SparkApplication* source =
      ::google::protobuf::DynamicCastToGenerated<SparkApplication>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:flyteidl.plugins.SparkApplication)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:flyteidl.plugins.SparkApplication)
    MergeFrom(*source);
  }
}

void SparkApplication::MergeFrom(const SparkApplication& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:flyteidl.plugins.SparkApplication)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void SparkApplication::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:flyteidl.plugins.SparkApplication)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void SparkApplication::CopyFrom(const SparkApplication& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:flyteidl.plugins.SparkApplication)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SparkApplication::IsInitialized() const {
  return true;
}

void SparkApplication::Swap(SparkApplication* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SparkApplication::InternalSwap(SparkApplication* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata SparkApplication::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto);
  return ::file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[kIndexInFileMessages];
}


// ===================================================================

SparkJob_SparkConfEntry_DoNotUse::SparkJob_SparkConfEntry_DoNotUse() {}
SparkJob_SparkConfEntry_DoNotUse::SparkJob_SparkConfEntry_DoNotUse(::google::protobuf::Arena* arena)
    : SuperType(arena) {}
void SparkJob_SparkConfEntry_DoNotUse::MergeFrom(const SparkJob_SparkConfEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::google::protobuf::Metadata SparkJob_SparkConfEntry_DoNotUse::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto);
  return ::file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[1];
}
void SparkJob_SparkConfEntry_DoNotUse::MergeFrom(
    const ::google::protobuf::Message& other) {
  ::google::protobuf::Message::MergeFrom(other);
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool SparkJob_SparkConfEntry_DoNotUse::_ParseMap(const char* begin, const char* end, void* object, ::google::protobuf::internal::ParseContext* ctx) {
  using MF = ::google::protobuf::internal::MapField<
      SparkJob_SparkConfEntry_DoNotUse, EntryKeyType, EntryValueType,
      kEntryKeyFieldType, kEntryValueFieldType,
      kEntryDefaultEnumValue>;
  auto mf = static_cast<MF*>(object);
  Parser<MF, ::google::protobuf::Map<EntryKeyType, EntryValueType>> parser(mf);
#define DO_(x) if (!(x)) return false
  DO_(parser.ParseMap(begin, end));
  DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
    parser.key().data(), static_cast<int>(parser.key().length()),
    ::google::protobuf::internal::WireFormatLite::PARSE,
    "flyteidl.plugins.SparkJob.SparkConfEntry.key"));
  DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
    parser.value().data(), static_cast<int>(parser.value().length()),
    ::google::protobuf::internal::WireFormatLite::PARSE,
    "flyteidl.plugins.SparkJob.SparkConfEntry.value"));
#undef DO_
  return true;
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER


// ===================================================================

SparkJob_HadoopConfEntry_DoNotUse::SparkJob_HadoopConfEntry_DoNotUse() {}
SparkJob_HadoopConfEntry_DoNotUse::SparkJob_HadoopConfEntry_DoNotUse(::google::protobuf::Arena* arena)
    : SuperType(arena) {}
void SparkJob_HadoopConfEntry_DoNotUse::MergeFrom(const SparkJob_HadoopConfEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::google::protobuf::Metadata SparkJob_HadoopConfEntry_DoNotUse::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto);
  return ::file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[2];
}
void SparkJob_HadoopConfEntry_DoNotUse::MergeFrom(
    const ::google::protobuf::Message& other) {
  ::google::protobuf::Message::MergeFrom(other);
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool SparkJob_HadoopConfEntry_DoNotUse::_ParseMap(const char* begin, const char* end, void* object, ::google::protobuf::internal::ParseContext* ctx) {
  using MF = ::google::protobuf::internal::MapField<
      SparkJob_HadoopConfEntry_DoNotUse, EntryKeyType, EntryValueType,
      kEntryKeyFieldType, kEntryValueFieldType,
      kEntryDefaultEnumValue>;
  auto mf = static_cast<MF*>(object);
  Parser<MF, ::google::protobuf::Map<EntryKeyType, EntryValueType>> parser(mf);
#define DO_(x) if (!(x)) return false
  DO_(parser.ParseMap(begin, end));
  DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
    parser.key().data(), static_cast<int>(parser.key().length()),
    ::google::protobuf::internal::WireFormatLite::PARSE,
    "flyteidl.plugins.SparkJob.HadoopConfEntry.key"));
  DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
    parser.value().data(), static_cast<int>(parser.value().length()),
    ::google::protobuf::internal::WireFormatLite::PARSE,
    "flyteidl.plugins.SparkJob.HadoopConfEntry.value"));
#undef DO_
  return true;
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER


// ===================================================================

void SparkJob::InitAsDefaultInstance() {
  ::flyteidl::plugins::_SparkJob_default_instance_._instance.get_mutable()->databricksconf_ = const_cast< ::google::protobuf::Struct*>(
      ::google::protobuf::Struct::internal_default_instance());
}
class SparkJob::HasBitSetters {
 public:
  static const ::google::protobuf::Struct& databricksconf(const SparkJob* msg);
};

const ::google::protobuf::Struct&
SparkJob::HasBitSetters::databricksconf(const SparkJob* msg) {
  return *msg->databricksconf_;
}
void SparkJob::clear_databricksconf() {
  if (GetArenaNoVirtual() == nullptr && databricksconf_ != nullptr) {
    delete databricksconf_;
  }
  databricksconf_ = nullptr;
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SparkJob::kApplicationTypeFieldNumber;
const int SparkJob::kMainApplicationFileFieldNumber;
const int SparkJob::kMainClassFieldNumber;
const int SparkJob::kSparkConfFieldNumber;
const int SparkJob::kHadoopConfFieldNumber;
const int SparkJob::kExecutorPathFieldNumber;
const int SparkJob::kDatabricksConfFieldNumber;
const int SparkJob::kDatabricksTokenFieldNumber;
const int SparkJob::kDatabricksInstanceFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SparkJob::SparkJob()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:flyteidl.plugins.SparkJob)
}
SparkJob::SparkJob(const SparkJob& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  sparkconf_.MergeFrom(from.sparkconf_);
  hadoopconf_.MergeFrom(from.hadoopconf_);
  mainapplicationfile_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.mainapplicationfile().size() > 0) {
    mainapplicationfile_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.mainapplicationfile_);
  }
  mainclass_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.mainclass().size() > 0) {
    mainclass_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.mainclass_);
  }
  executorpath_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.executorpath().size() > 0) {
    executorpath_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.executorpath_);
  }
  databrickstoken_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.databrickstoken().size() > 0) {
    databrickstoken_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.databrickstoken_);
  }
  databricksinstance_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.databricksinstance().size() > 0) {
    databricksinstance_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.databricksinstance_);
  }
  if (from.has_databricksconf()) {
    databricksconf_ = new ::google::protobuf::Struct(*from.databricksconf_);
  } else {
    databricksconf_ = nullptr;
  }
  applicationtype_ = from.applicationtype_;
  // @@protoc_insertion_point(copy_constructor:flyteidl.plugins.SparkJob)
}

void SparkJob::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_SparkJob_flyteidl_2fplugins_2fspark_2eproto.base);
  mainapplicationfile_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  mainclass_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  executorpath_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  databrickstoken_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  databricksinstance_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&databricksconf_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&applicationtype_) -
      reinterpret_cast<char*>(&databricksconf_)) + sizeof(applicationtype_));
}

SparkJob::~SparkJob() {
  // @@protoc_insertion_point(destructor:flyteidl.plugins.SparkJob)
  SharedDtor();
}

void SparkJob::SharedDtor() {
  mainapplicationfile_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  mainclass_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  executorpath_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  databrickstoken_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  databricksinstance_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete databricksconf_;
}

void SparkJob::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const SparkJob& SparkJob::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_SparkJob_flyteidl_2fplugins_2fspark_2eproto.base);
  return *internal_default_instance();
}


void SparkJob::Clear() {
// @@protoc_insertion_point(message_clear_start:flyteidl.plugins.SparkJob)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  sparkconf_.Clear();
  hadoopconf_.Clear();
  mainapplicationfile_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  mainclass_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  executorpath_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  databrickstoken_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  databricksinstance_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (GetArenaNoVirtual() == nullptr && databricksconf_ != nullptr) {
    delete databricksconf_;
  }
  databricksconf_ = nullptr;
  applicationtype_ = 0;
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* SparkJob::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<SparkJob*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // .flyteidl.plugins.SparkApplication.Type applicationType = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 8) goto handle_unusual;
        ::google::protobuf::uint64 val = ::google::protobuf::internal::ReadVarint(&ptr);
        msg->set_applicationtype(static_cast<::flyteidl::plugins::SparkApplication_Type>(val));
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        break;
      }
      // string mainApplicationFile = 2;
      case 2: {
        if (static_cast<::google::protobuf::uint8>(tag) != 18) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        ctx->extra_parse_data().SetFieldName("flyteidl.plugins.SparkJob.mainApplicationFile");
        object = msg->mutable_mainapplicationfile();
        if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
          parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
          goto string_till_end;
        }
        GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
        ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
        ptr += size;
        break;
      }
      // string mainClass = 3;
      case 3: {
        if (static_cast<::google::protobuf::uint8>(tag) != 26) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        ctx->extra_parse_data().SetFieldName("flyteidl.plugins.SparkJob.mainClass");
        object = msg->mutable_mainclass();
        if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
          parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
          goto string_till_end;
        }
        GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
        ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
        ptr += size;
        break;
      }
      // map<string, string> sparkConf = 4;
      case 4: {
        if (static_cast<::google::protobuf::uint8>(tag) != 34) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          parser_till_end = ::google::protobuf::internal::SlowMapEntryParser;
          auto parse_map = ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse::_ParseMap;
          ctx->extra_parse_data().payload.clear();
          ctx->extra_parse_data().parse_map = parse_map;
          object = &msg->sparkconf_;
          if (size > end - ptr) goto len_delim_till_end;
          auto newend = ptr + size;
          GOOGLE_PROTOBUF_PARSER_ASSERT(parse_map(ptr, newend, object, ctx));
          ptr = newend;
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 34 && (ptr += 1));
        break;
      }
      // map<string, string> hadoopConf = 5;
      case 5: {
        if (static_cast<::google::protobuf::uint8>(tag) != 42) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          parser_till_end = ::google::protobuf::internal::SlowMapEntryParser;
          auto parse_map = ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse::_ParseMap;
          ctx->extra_parse_data().payload.clear();
          ctx->extra_parse_data().parse_map = parse_map;
          object = &msg->hadoopconf_;
          if (size > end - ptr) goto len_delim_till_end;
          auto newend = ptr + size;
          GOOGLE_PROTOBUF_PARSER_ASSERT(parse_map(ptr, newend, object, ctx));
          ptr = newend;
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 42 && (ptr += 1));
        break;
      }
      // string executorPath = 6;
      case 6: {
        if (static_cast<::google::protobuf::uint8>(tag) != 50) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        ctx->extra_parse_data().SetFieldName("flyteidl.plugins.SparkJob.executorPath");
        object = msg->mutable_executorpath();
        if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
          parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
          goto string_till_end;
        }
        GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
        ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
        ptr += size;
        break;
      }
      // .google.protobuf.Struct databricksConf = 7;
      case 7: {
        if (static_cast<::google::protobuf::uint8>(tag) != 58) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::google::protobuf::Struct::_InternalParse;
        object = msg->mutable_databricksconf();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      // string databricksToken = 8;
      case 8: {
        if (static_cast<::google::protobuf::uint8>(tag) != 66) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        ctx->extra_parse_data().SetFieldName("flyteidl.plugins.SparkJob.databricksToken");
        object = msg->mutable_databrickstoken();
        if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
          parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
          goto string_till_end;
        }
        GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
        ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
        ptr += size;
        break;
      }
      // string databricksInstance = 9;
      case 9: {
        if (static_cast<::google::protobuf::uint8>(tag) != 74) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        ctx->extra_parse_data().SetFieldName("flyteidl.plugins.SparkJob.databricksInstance");
        object = msg->mutable_databricksinstance();
        if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
          parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
          goto string_till_end;
        }
        GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
        ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
        ptr += size;
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
string_till_end:
  static_cast<::std::string*>(object)->clear();
  static_cast<::std::string*>(object)->reserve(size);
  goto len_delim_till_end;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool SparkJob::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:flyteidl.plugins.SparkJob)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .flyteidl.plugins.SparkApplication.Type applicationType = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (8 & 0xFF)) {
          int value = 0;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          set_applicationtype(static_cast< ::flyteidl::plugins::SparkApplication_Type >(value));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string mainApplicationFile = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (18 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_mainapplicationfile()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->mainapplicationfile().data(), static_cast<int>(this->mainapplicationfile().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.mainApplicationFile"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string mainClass = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (26 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_mainclass()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->mainclass().data(), static_cast<int>(this->mainclass().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.mainClass"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // map<string, string> sparkConf = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (34 & 0xFF)) {
          SparkJob_SparkConfEntry_DoNotUse::Parser< ::google::protobuf::internal::MapField<
              SparkJob_SparkConfEntry_DoNotUse,
              ::std::string, ::std::string,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              0 >,
            ::google::protobuf::Map< ::std::string, ::std::string > > parser(&sparkconf_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), static_cast<int>(parser.key().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.SparkConfEntry.key"));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.value().data(), static_cast<int>(parser.value().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.SparkConfEntry.value"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // map<string, string> hadoopConf = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (42 & 0xFF)) {
          SparkJob_HadoopConfEntry_DoNotUse::Parser< ::google::protobuf::internal::MapField<
              SparkJob_HadoopConfEntry_DoNotUse,
              ::std::string, ::std::string,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              ::google::protobuf::internal::WireFormatLite::TYPE_STRING,
              0 >,
            ::google::protobuf::Map< ::std::string, ::std::string > > parser(&hadoopconf_);
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
              input, &parser));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.key().data(), static_cast<int>(parser.key().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.HadoopConfEntry.key"));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            parser.value().data(), static_cast<int>(parser.value().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.HadoopConfEntry.value"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string executorPath = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (50 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_executorpath()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->executorpath().data(), static_cast<int>(this->executorpath().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.executorPath"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .google.protobuf.Struct databricksConf = 7;
      case 7: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (58 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_databricksconf()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string databricksToken = 8;
      case 8: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (66 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_databrickstoken()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->databrickstoken().data(), static_cast<int>(this->databrickstoken().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.databricksToken"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string databricksInstance = 9;
      case 9: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (74 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_databricksinstance()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->databricksinstance().data(), static_cast<int>(this->databricksinstance().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "flyteidl.plugins.SparkJob.databricksInstance"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:flyteidl.plugins.SparkJob)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:flyteidl.plugins.SparkJob)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void SparkJob::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:flyteidl.plugins.SparkJob)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .flyteidl.plugins.SparkApplication.Type applicationType = 1;
  if (this->applicationtype() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->applicationtype(), output);
  }

  // string mainApplicationFile = 2;
  if (this->mainapplicationfile().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->mainapplicationfile().data(), static_cast<int>(this->mainapplicationfile().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.mainApplicationFile");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->mainapplicationfile(), output);
  }

  // string mainClass = 3;
  if (this->mainclass().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->mainclass().data(), static_cast<int>(this->mainclass().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.mainClass");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      3, this->mainclass(), output);
  }

  // map<string, string> sparkConf = 4;
  if (!this->sparkconf().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::std::string >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.SparkConfEntry.key");
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->second.data(), static_cast<int>(p->second.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.SparkConfEntry.value");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->sparkconf().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->sparkconf().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::std::string >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->sparkconf().begin();
          it != this->sparkconf().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<SparkJob_SparkConfEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(sparkconf_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(4, *entry, output);
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<SparkJob_SparkConfEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->sparkconf().begin();
          it != this->sparkconf().end(); ++it) {
        entry.reset(sparkconf_.NewEntryWrapper(it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(4, *entry, output);
        Utf8Check::Check(&(*it));
      }
    }
  }

  // map<string, string> hadoopConf = 5;
  if (!this->hadoopconf().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::std::string >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.HadoopConfEntry.key");
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->second.data(), static_cast<int>(p->second.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.HadoopConfEntry.value");
      }
    };

    if (output->IsSerializationDeterministic() &&
        this->hadoopconf().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->hadoopconf().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::std::string >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->hadoopconf().begin();
          it != this->hadoopconf().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<SparkJob_HadoopConfEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(hadoopconf_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(5, *entry, output);
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<SparkJob_HadoopConfEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->hadoopconf().begin();
          it != this->hadoopconf().end(); ++it) {
        entry.reset(hadoopconf_.NewEntryWrapper(it->first, it->second));
        ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(5, *entry, output);
        Utf8Check::Check(&(*it));
      }
    }
  }

  // string executorPath = 6;
  if (this->executorpath().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->executorpath().data(), static_cast<int>(this->executorpath().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.executorPath");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      6, this->executorpath(), output);
  }

  // .google.protobuf.Struct databricksConf = 7;
  if (this->has_databricksconf()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      7, HasBitSetters::databricksconf(this), output);
  }

  // string databricksToken = 8;
  if (this->databrickstoken().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->databrickstoken().data(), static_cast<int>(this->databrickstoken().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.databricksToken");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      8, this->databrickstoken(), output);
  }

  // string databricksInstance = 9;
  if (this->databricksinstance().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->databricksinstance().data(), static_cast<int>(this->databricksinstance().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.databricksInstance");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      9, this->databricksinstance(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:flyteidl.plugins.SparkJob)
}

::google::protobuf::uint8* SparkJob::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:flyteidl.plugins.SparkJob)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .flyteidl.plugins.SparkApplication.Type applicationType = 1;
  if (this->applicationtype() != 0) {
    target = ::google::protobuf::internal::WireFormatLite::WriteEnumToArray(
      1, this->applicationtype(), target);
  }

  // string mainApplicationFile = 2;
  if (this->mainapplicationfile().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->mainapplicationfile().data(), static_cast<int>(this->mainapplicationfile().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.mainApplicationFile");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->mainapplicationfile(), target);
  }

  // string mainClass = 3;
  if (this->mainclass().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->mainclass().data(), static_cast<int>(this->mainclass().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.mainClass");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        3, this->mainclass(), target);
  }

  // map<string, string> sparkConf = 4;
  if (!this->sparkconf().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::std::string >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.SparkConfEntry.key");
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->second.data(), static_cast<int>(p->second.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.SparkConfEntry.value");
      }
    };

    if (false &&
        this->sparkconf().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->sparkconf().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::std::string >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->sparkconf().begin();
          it != this->sparkconf().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<SparkJob_SparkConfEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(sparkconf_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(4, *entry, target);
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<SparkJob_SparkConfEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->sparkconf().begin();
          it != this->sparkconf().end(); ++it) {
        entry.reset(sparkconf_.NewEntryWrapper(it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(4, *entry, target);
        Utf8Check::Check(&(*it));
      }
    }
  }

  // map<string, string> hadoopConf = 5;
  if (!this->hadoopconf().empty()) {
    typedef ::google::protobuf::Map< ::std::string, ::std::string >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::google::protobuf::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.HadoopConfEntry.key");
        ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
          p->second.data(), static_cast<int>(p->second.length()),
          ::google::protobuf::internal::WireFormatLite::SERIALIZE,
          "flyteidl.plugins.SparkJob.HadoopConfEntry.value");
      }
    };

    if (false &&
        this->hadoopconf().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->hadoopconf().size()]);
      typedef ::google::protobuf::Map< ::std::string, ::std::string >::size_type size_type;
      size_type n = 0;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->hadoopconf().begin();
          it != this->hadoopconf().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      ::std::unique_ptr<SparkJob_HadoopConfEntry_DoNotUse> entry;
      for (size_type i = 0; i < n; i++) {
        entry.reset(hadoopconf_.NewEntryWrapper(items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(5, *entry, target);
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      ::std::unique_ptr<SparkJob_HadoopConfEntry_DoNotUse> entry;
      for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
          it = this->hadoopconf().begin();
          it != this->hadoopconf().end(); ++it) {
        entry.reset(hadoopconf_.NewEntryWrapper(it->first, it->second));
        target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessageNoVirtualToArray(5, *entry, target);
        Utf8Check::Check(&(*it));
      }
    }
  }

  // string executorPath = 6;
  if (this->executorpath().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->executorpath().data(), static_cast<int>(this->executorpath().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.executorPath");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        6, this->executorpath(), target);
  }

  // .google.protobuf.Struct databricksConf = 7;
  if (this->has_databricksconf()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        7, HasBitSetters::databricksconf(this), target);
  }

  // string databricksToken = 8;
  if (this->databrickstoken().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->databrickstoken().data(), static_cast<int>(this->databrickstoken().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.databricksToken");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        8, this->databrickstoken(), target);
  }

  // string databricksInstance = 9;
  if (this->databricksinstance().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->databricksinstance().data(), static_cast<int>(this->databricksinstance().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "flyteidl.plugins.SparkJob.databricksInstance");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        9, this->databricksinstance(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:flyteidl.plugins.SparkJob)
  return target;
}

size_t SparkJob::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:flyteidl.plugins.SparkJob)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<string, string> sparkConf = 4;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->sparkconf_size());
  {
    ::std::unique_ptr<SparkJob_SparkConfEntry_DoNotUse> entry;
    for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
        it = this->sparkconf().begin();
        it != this->sparkconf().end(); ++it) {
      entry.reset(sparkconf_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
  }

  // map<string, string> hadoopConf = 5;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->hadoopconf_size());
  {
    ::std::unique_ptr<SparkJob_HadoopConfEntry_DoNotUse> entry;
    for (::google::protobuf::Map< ::std::string, ::std::string >::const_iterator
        it = this->hadoopconf().begin();
        it != this->hadoopconf().end(); ++it) {
      entry.reset(hadoopconf_.NewEntryWrapper(it->first, it->second));
      total_size += ::google::protobuf::internal::WireFormatLite::
          MessageSizeNoVirtual(*entry);
    }
  }

  // string mainApplicationFile = 2;
  if (this->mainapplicationfile().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->mainapplicationfile());
  }

  // string mainClass = 3;
  if (this->mainclass().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->mainclass());
  }

  // string executorPath = 6;
  if (this->executorpath().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->executorpath());
  }

  // string databricksToken = 8;
  if (this->databrickstoken().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->databrickstoken());
  }

  // string databricksInstance = 9;
  if (this->databricksinstance().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->databricksinstance());
  }

  // .google.protobuf.Struct databricksConf = 7;
  if (this->has_databricksconf()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *databricksconf_);
  }

  // .flyteidl.plugins.SparkApplication.Type applicationType = 1;
  if (this->applicationtype() != 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->applicationtype());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void SparkJob::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:flyteidl.plugins.SparkJob)
  GOOGLE_DCHECK_NE(&from, this);
  const SparkJob* source =
      ::google::protobuf::DynamicCastToGenerated<SparkJob>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:flyteidl.plugins.SparkJob)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:flyteidl.plugins.SparkJob)
    MergeFrom(*source);
  }
}

void SparkJob::MergeFrom(const SparkJob& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:flyteidl.plugins.SparkJob)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  sparkconf_.MergeFrom(from.sparkconf_);
  hadoopconf_.MergeFrom(from.hadoopconf_);
  if (from.mainapplicationfile().size() > 0) {

    mainapplicationfile_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.mainapplicationfile_);
  }
  if (from.mainclass().size() > 0) {

    mainclass_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.mainclass_);
  }
  if (from.executorpath().size() > 0) {

    executorpath_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.executorpath_);
  }
  if (from.databrickstoken().size() > 0) {

    databrickstoken_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.databrickstoken_);
  }
  if (from.databricksinstance().size() > 0) {

    databricksinstance_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.databricksinstance_);
  }
  if (from.has_databricksconf()) {
    mutable_databricksconf()->::google::protobuf::Struct::MergeFrom(from.databricksconf());
  }
  if (from.applicationtype() != 0) {
    set_applicationtype(from.applicationtype());
  }
}

void SparkJob::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:flyteidl.plugins.SparkJob)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void SparkJob::CopyFrom(const SparkJob& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:flyteidl.plugins.SparkJob)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SparkJob::IsInitialized() const {
  return true;
}

void SparkJob::Swap(SparkJob* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SparkJob::InternalSwap(SparkJob* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  sparkconf_.Swap(&other->sparkconf_);
  hadoopconf_.Swap(&other->hadoopconf_);
  mainapplicationfile_.Swap(&other->mainapplicationfile_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  mainclass_.Swap(&other->mainclass_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  executorpath_.Swap(&other->executorpath_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  databrickstoken_.Swap(&other->databrickstoken_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  databricksinstance_.Swap(&other->databricksinstance_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(databricksconf_, other->databricksconf_);
  swap(applicationtype_, other->applicationtype_);
}

::google::protobuf::Metadata SparkJob::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_flyteidl_2fplugins_2fspark_2eproto);
  return ::file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[kIndexInFileMessages];
}


// @@protoc_insertion_point(namespace_scope)
}  // namespace plugins
}  // namespace flyteidl
namespace google {
namespace protobuf {
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkApplication* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkApplication >(Arena* arena) {
  return Arena::CreateInternal< ::flyteidl::plugins::SparkApplication >(arena);
}
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateInternal< ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateInternal< ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkJob* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkJob >(Arena* arena) {
  return Arena::CreateInternal< ::flyteidl::plugins::SparkJob >(arena);
}
}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
