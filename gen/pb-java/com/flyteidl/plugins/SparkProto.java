// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: flyteidl/plugins/spark.proto

package com.flyteidl.plugins;

public final class SparkProto {
  private SparkProto() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  public interface SparkApplicationOrBuilder extends
      // @@protoc_insertion_point(interface_extends:flyteidl.plugins.SparkApplication)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code flyteidl.plugins.SparkApplication}
   */
  public static final class SparkApplication extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:flyteidl.plugins.SparkApplication)
      SparkApplicationOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkApplication.newBuilder() to construct.
    private SparkApplication(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkApplication() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkApplication();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkApplication_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              com.flyteidl.plugins.SparkProto.SparkApplication.class, com.flyteidl.plugins.SparkProto.SparkApplication.Builder.class);
    }

    /**
     * Protobuf enum {@code flyteidl.plugins.SparkApplication.Type}
     */
    public enum Type
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>PYTHON = 0;</code>
       */
      PYTHON(0),
      /**
       * <code>JAVA = 1;</code>
       */
      JAVA(1),
      /**
       * <code>SCALA = 2;</code>
       */
      SCALA(2),
      /**
       * <code>R = 3;</code>
       */
      R(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <code>PYTHON = 0;</code>
       */
      public static final int PYTHON_VALUE = 0;
      /**
       * <code>JAVA = 1;</code>
       */
      public static final int JAVA_VALUE = 1;
      /**
       * <code>SCALA = 2;</code>
       */
      public static final int SCALA_VALUE = 2;
      /**
       * <code>R = 3;</code>
       */
      public static final int R_VALUE = 3;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Type valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Type forNumber(int value) {
        switch (value) {
          case 0: return PYTHON;
          case 1: return JAVA;
          case 2: return SCALA;
          case 3: return R;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Type>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Type> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Type>() {
              public Type findValueByNumber(int number) {
                return Type.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return com.flyteidl.plugins.SparkProto.SparkApplication.getDescriptor().getEnumTypes().get(0);
      }

      private static final Type[] VALUES = values();

      public static Type valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Type(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:flyteidl.plugins.SparkApplication.Type)
    }

    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static com.flyteidl.plugins.SparkProto.SparkApplication parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(com.flyteidl.plugins.SparkProto.SparkApplication prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code flyteidl.plugins.SparkApplication}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:flyteidl.plugins.SparkApplication)
        com.flyteidl.plugins.SparkProto.SparkApplicationOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkApplication_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                com.flyteidl.plugins.SparkProto.SparkApplication.class, com.flyteidl.plugins.SparkProto.SparkApplication.Builder.class);
      }

      // Construct using com.flyteidl.plugins.SparkProto.SparkApplication.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkApplication_descriptor;
      }

      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkApplication getDefaultInstanceForType() {
        return com.flyteidl.plugins.SparkProto.SparkApplication.getDefaultInstance();
      }

      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkApplication build() {
        com.flyteidl.plugins.SparkProto.SparkApplication result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkApplication buildPartial() {
        com.flyteidl.plugins.SparkProto.SparkApplication result = new com.flyteidl.plugins.SparkProto.SparkApplication(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:flyteidl.plugins.SparkApplication)
    }

    // @@protoc_insertion_point(class_scope:flyteidl.plugins.SparkApplication)
    private static final com.flyteidl.plugins.SparkProto.SparkApplication DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new com.flyteidl.plugins.SparkProto.SparkApplication();
    }

    public static com.flyteidl.plugins.SparkProto.SparkApplication getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkApplication>
        PARSER = new com.google.protobuf.AbstractParser<SparkApplication>() {
      @java.lang.Override
      public SparkApplication parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(
                  builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SparkApplication> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkApplication> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public com.flyteidl.plugins.SparkProto.SparkApplication getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkJobOrBuilder extends
      // @@protoc_insertion_point(interface_extends:flyteidl.plugins.SparkJob)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
     * @return The enum numeric value on the wire for applicationType.
     */
    int getApplicationTypeValue();
    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
     * @return The applicationType.
     */
    com.flyteidl.plugins.SparkProto.SparkApplication.Type getApplicationType();

    /**
     * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
     * @return The mainApplicationFile.
     */
    java.lang.String getMainApplicationFile();
    /**
     * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
     * @return The bytes for mainApplicationFile.
     */
    com.google.protobuf.ByteString
        getMainApplicationFileBytes();

    /**
     * <code>string mainClass = 3 [json_name = "mainClass"];</code>
     * @return The mainClass.
     */
    java.lang.String getMainClass();
    /**
     * <code>string mainClass = 3 [json_name = "mainClass"];</code>
     * @return The bytes for mainClass.
     */
    com.google.protobuf.ByteString
        getMainClassBytes();

    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */
    int getSparkConfCount();
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */
    boolean containsSparkConf(
        java.lang.String key);
    /**
     * Use {@link #getSparkConfMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getSparkConf();
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getSparkConfMap();
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */

    java.lang.String getSparkConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */

    java.lang.String getSparkConfOrThrow(
        java.lang.String key);

    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */
    int getHadoopConfCount();
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */
    boolean containsHadoopConf(
        java.lang.String key);
    /**
     * Use {@link #getHadoopConfMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getHadoopConf();
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getHadoopConfMap();
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */

    java.lang.String getHadoopConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */

    java.lang.String getHadoopConfOrThrow(
        java.lang.String key);

    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6 [json_name = "executorPath"];</code>
     * @return The executorPath.
     */
    java.lang.String getExecutorPath();
    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6 [json_name = "executorPath"];</code>
     * @return The bytes for executorPath.
     */
    com.google.protobuf.ByteString
        getExecutorPathBytes();
  }
  /**
   * <pre>
   * Custom Proto for Spark Plugin.
   * </pre>
   *
   * Protobuf type {@code flyteidl.plugins.SparkJob}
   */
  public static final class SparkJob extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:flyteidl.plugins.SparkJob)
      SparkJobOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkJob.newBuilder() to construct.
    private SparkJob(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkJob() {
      applicationType_ = 0;
      mainApplicationFile_ = "";
      mainClass_ = "";
      executorPath_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkJob();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 4:
          return internalGetSparkConf();
        case 5:
          return internalGetHadoopConf();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              com.flyteidl.plugins.SparkProto.SparkJob.class, com.flyteidl.plugins.SparkProto.SparkJob.Builder.class);
    }

    public static final int APPLICATIONTYPE_FIELD_NUMBER = 1;
    private int applicationType_;
    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
     * @return The enum numeric value on the wire for applicationType.
     */
    @java.lang.Override public int getApplicationTypeValue() {
      return applicationType_;
    }
    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
     * @return The applicationType.
     */
    @java.lang.Override public com.flyteidl.plugins.SparkProto.SparkApplication.Type getApplicationType() {
      @SuppressWarnings("deprecation")
      com.flyteidl.plugins.SparkProto.SparkApplication.Type result = com.flyteidl.plugins.SparkProto.SparkApplication.Type.valueOf(applicationType_);
      return result == null ? com.flyteidl.plugins.SparkProto.SparkApplication.Type.UNRECOGNIZED : result;
    }

    public static final int MAINAPPLICATIONFILE_FIELD_NUMBER = 2;
    private volatile java.lang.Object mainApplicationFile_;
    /**
     * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
     * @return The mainApplicationFile.
     */
    @java.lang.Override
    public java.lang.String getMainApplicationFile() {
      java.lang.Object ref = mainApplicationFile_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        mainApplicationFile_ = s;
        return s;
      }
    }
    /**
     * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
     * @return The bytes for mainApplicationFile.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getMainApplicationFileBytes() {
      java.lang.Object ref = mainApplicationFile_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mainApplicationFile_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MAINCLASS_FIELD_NUMBER = 3;
    private volatile java.lang.Object mainClass_;
    /**
     * <code>string mainClass = 3 [json_name = "mainClass"];</code>
     * @return The mainClass.
     */
    @java.lang.Override
    public java.lang.String getMainClass() {
      java.lang.Object ref = mainClass_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        mainClass_ = s;
        return s;
      }
    }
    /**
     * <code>string mainClass = 3 [json_name = "mainClass"];</code>
     * @return The bytes for mainClass.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getMainClassBytes() {
      java.lang.Object ref = mainClass_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mainClass_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SPARKCONF_FIELD_NUMBER = 4;
    private static final class SparkConfDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> sparkConf_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetSparkConf() {
      if (sparkConf_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            SparkConfDefaultEntryHolder.defaultEntry);
      }
      return sparkConf_;
    }

    public int getSparkConfCount() {
      return internalGetSparkConf().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */

    @java.lang.Override
    public boolean containsSparkConf(
        java.lang.String key) {
      if (key == null) { throw new NullPointerException("map key"); }
      return internalGetSparkConf().getMap().containsKey(key);
    }
    /**
     * Use {@link #getSparkConfMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getSparkConf() {
      return getSparkConfMap();
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getSparkConfMap() {
      return internalGetSparkConf().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */
    @java.lang.Override

    public java.lang.String getSparkConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new NullPointerException("map key"); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetSparkConf().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
     */
    @java.lang.Override

    public java.lang.String getSparkConfOrThrow(
        java.lang.String key) {
      if (key == null) { throw new NullPointerException("map key"); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetSparkConf().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int HADOOPCONF_FIELD_NUMBER = 5;
    private static final class HadoopConfDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> hadoopConf_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetHadoopConf() {
      if (hadoopConf_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            HadoopConfDefaultEntryHolder.defaultEntry);
      }
      return hadoopConf_;
    }

    public int getHadoopConfCount() {
      return internalGetHadoopConf().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */

    @java.lang.Override
    public boolean containsHadoopConf(
        java.lang.String key) {
      if (key == null) { throw new NullPointerException("map key"); }
      return internalGetHadoopConf().getMap().containsKey(key);
    }
    /**
     * Use {@link #getHadoopConfMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getHadoopConf() {
      return getHadoopConfMap();
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getHadoopConfMap() {
      return internalGetHadoopConf().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */
    @java.lang.Override

    public java.lang.String getHadoopConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new NullPointerException("map key"); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetHadoopConf().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
     */
    @java.lang.Override

    public java.lang.String getHadoopConfOrThrow(
        java.lang.String key) {
      if (key == null) { throw new NullPointerException("map key"); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetHadoopConf().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int EXECUTORPATH_FIELD_NUMBER = 6;
    private volatile java.lang.Object executorPath_;
    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6 [json_name = "executorPath"];</code>
     * @return The executorPath.
     */
    @java.lang.Override
    public java.lang.String getExecutorPath() {
      java.lang.Object ref = executorPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorPath_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6 [json_name = "executorPath"];</code>
     * @return The bytes for executorPath.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getExecutorPathBytes() {
      java.lang.Object ref = executorPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static com.flyteidl.plugins.SparkProto.SparkJob parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(com.flyteidl.plugins.SparkProto.SparkJob prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Custom Proto for Spark Plugin.
     * </pre>
     *
     * Protobuf type {@code flyteidl.plugins.SparkJob}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:flyteidl.plugins.SparkJob)
        com.flyteidl.plugins.SparkProto.SparkJobOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetSparkConf();
          case 5:
            return internalGetHadoopConf();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetMutableSparkConf();
          case 5:
            return internalGetMutableHadoopConf();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                com.flyteidl.plugins.SparkProto.SparkJob.class, com.flyteidl.plugins.SparkProto.SparkJob.Builder.class);
      }

      // Construct using com.flyteidl.plugins.SparkProto.SparkJob.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        applicationType_ = 0;

        mainApplicationFile_ = "";

        mainClass_ = "";

        internalGetMutableSparkConf().clear();
        internalGetMutableHadoopConf().clear();
        executorPath_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return com.flyteidl.plugins.SparkProto.internal_static_flyteidl_plugins_SparkJob_descriptor;
      }

      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkJob getDefaultInstanceForType() {
        return com.flyteidl.plugins.SparkProto.SparkJob.getDefaultInstance();
      }

      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkJob build() {
        com.flyteidl.plugins.SparkProto.SparkJob result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkJob buildPartial() {
        com.flyteidl.plugins.SparkProto.SparkJob result = new com.flyteidl.plugins.SparkProto.SparkJob(this);
        int from_bitField0_ = bitField0_;
        result.applicationType_ = applicationType_;
        result.mainApplicationFile_ = mainApplicationFile_;
        result.mainClass_ = mainClass_;
        result.sparkConf_ = internalGetSparkConf();
        result.sparkConf_.makeImmutable();
        result.hadoopConf_ = internalGetHadoopConf();
        result.hadoopConf_.makeImmutable();
        result.executorPath_ = executorPath_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      private int bitField0_;

      private int applicationType_ = 0;
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
       * @return The enum numeric value on the wire for applicationType.
       */
      @java.lang.Override public int getApplicationTypeValue() {
        return applicationType_;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
       * @param value The enum numeric value on the wire for applicationType to set.
       * @return This builder for chaining.
       */
      public Builder setApplicationTypeValue(int value) {
        
        applicationType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
       * @return The applicationType.
       */
      @java.lang.Override
      public com.flyteidl.plugins.SparkProto.SparkApplication.Type getApplicationType() {
        @SuppressWarnings("deprecation")
        com.flyteidl.plugins.SparkProto.SparkApplication.Type result = com.flyteidl.plugins.SparkProto.SparkApplication.Type.valueOf(applicationType_);
        return result == null ? com.flyteidl.plugins.SparkProto.SparkApplication.Type.UNRECOGNIZED : result;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
       * @param value The applicationType to set.
       * @return This builder for chaining.
       */
      public Builder setApplicationType(com.flyteidl.plugins.SparkProto.SparkApplication.Type value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        applicationType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1 [json_name = "applicationType"];</code>
       * @return This builder for chaining.
       */
      public Builder clearApplicationType() {
        
        applicationType_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object mainApplicationFile_ = "";
      /**
       * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
       * @return The mainApplicationFile.
       */
      public java.lang.String getMainApplicationFile() {
        java.lang.Object ref = mainApplicationFile_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          mainApplicationFile_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
       * @return The bytes for mainApplicationFile.
       */
      public com.google.protobuf.ByteString
          getMainApplicationFileBytes() {
        java.lang.Object ref = mainApplicationFile_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mainApplicationFile_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
       * @param value The mainApplicationFile to set.
       * @return This builder for chaining.
       */
      public Builder setMainApplicationFile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        mainApplicationFile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
       * @return This builder for chaining.
       */
      public Builder clearMainApplicationFile() {
        
        mainApplicationFile_ = getDefaultInstance().getMainApplicationFile();
        onChanged();
        return this;
      }
      /**
       * <code>string mainApplicationFile = 2 [json_name = "mainApplicationFile"];</code>
       * @param value The bytes for mainApplicationFile to set.
       * @return This builder for chaining.
       */
      public Builder setMainApplicationFileBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        mainApplicationFile_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object mainClass_ = "";
      /**
       * <code>string mainClass = 3 [json_name = "mainClass"];</code>
       * @return The mainClass.
       */
      public java.lang.String getMainClass() {
        java.lang.Object ref = mainClass_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          mainClass_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string mainClass = 3 [json_name = "mainClass"];</code>
       * @return The bytes for mainClass.
       */
      public com.google.protobuf.ByteString
          getMainClassBytes() {
        java.lang.Object ref = mainClass_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mainClass_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string mainClass = 3 [json_name = "mainClass"];</code>
       * @param value The mainClass to set.
       * @return This builder for chaining.
       */
      public Builder setMainClass(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        mainClass_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string mainClass = 3 [json_name = "mainClass"];</code>
       * @return This builder for chaining.
       */
      public Builder clearMainClass() {
        
        mainClass_ = getDefaultInstance().getMainClass();
        onChanged();
        return this;
      }
      /**
       * <code>string mainClass = 3 [json_name = "mainClass"];</code>
       * @param value The bytes for mainClass to set.
       * @return This builder for chaining.
       */
      public Builder setMainClassBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        mainClass_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> sparkConf_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetSparkConf() {
        if (sparkConf_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              SparkConfDefaultEntryHolder.defaultEntry);
        }
        return sparkConf_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableSparkConf() {
        onChanged();;
        if (sparkConf_ == null) {
          sparkConf_ = com.google.protobuf.MapField.newMapField(
              SparkConfDefaultEntryHolder.defaultEntry);
        }
        if (!sparkConf_.isMutable()) {
          sparkConf_ = sparkConf_.copy();
        }
        return sparkConf_;
      }

      public int getSparkConfCount() {
        return internalGetSparkConf().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */

      @java.lang.Override
      public boolean containsSparkConf(
          java.lang.String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        return internalGetSparkConf().getMap().containsKey(key);
      }
      /**
       * Use {@link #getSparkConfMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getSparkConf() {
        return getSparkConfMap();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getSparkConfMap() {
        return internalGetSparkConf().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */
      @java.lang.Override

      public java.lang.String getSparkConfOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new NullPointerException("map key"); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetSparkConf().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */
      @java.lang.Override

      public java.lang.String getSparkConfOrThrow(
          java.lang.String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetSparkConf().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearSparkConf() {
        internalGetMutableSparkConf().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */

      public Builder removeSparkConf(
          java.lang.String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        internalGetMutableSparkConf().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableSparkConf() {
        return internalGetMutableSparkConf().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */
      public Builder putSparkConf(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new NullPointerException("map key"); }
        if (value == null) {
  throw new NullPointerException("map value");
}

        internalGetMutableSparkConf().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4 [json_name = "sparkConf"];</code>
       */

      public Builder putAllSparkConf(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableSparkConf().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> hadoopConf_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetHadoopConf() {
        if (hadoopConf_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              HadoopConfDefaultEntryHolder.defaultEntry);
        }
        return hadoopConf_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableHadoopConf() {
        onChanged();;
        if (hadoopConf_ == null) {
          hadoopConf_ = com.google.protobuf.MapField.newMapField(
              HadoopConfDefaultEntryHolder.defaultEntry);
        }
        if (!hadoopConf_.isMutable()) {
          hadoopConf_ = hadoopConf_.copy();
        }
        return hadoopConf_;
      }

      public int getHadoopConfCount() {
        return internalGetHadoopConf().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */

      @java.lang.Override
      public boolean containsHadoopConf(
          java.lang.String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        return internalGetHadoopConf().getMap().containsKey(key);
      }
      /**
       * Use {@link #getHadoopConfMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getHadoopConf() {
        return getHadoopConfMap();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getHadoopConfMap() {
        return internalGetHadoopConf().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */
      @java.lang.Override

      public java.lang.String getHadoopConfOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new NullPointerException("map key"); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetHadoopConf().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */
      @java.lang.Override

      public java.lang.String getHadoopConfOrThrow(
          java.lang.String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetHadoopConf().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearHadoopConf() {
        internalGetMutableHadoopConf().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */

      public Builder removeHadoopConf(
          java.lang.String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        internalGetMutableHadoopConf().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableHadoopConf() {
        return internalGetMutableHadoopConf().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */
      public Builder putHadoopConf(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new NullPointerException("map key"); }
        if (value == null) {
  throw new NullPointerException("map value");
}

        internalGetMutableHadoopConf().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5 [json_name = "hadoopConf"];</code>
       */

      public Builder putAllHadoopConf(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableHadoopConf().getMutableMap()
            .putAll(values);
        return this;
      }

      private java.lang.Object executorPath_ = "";
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6 [json_name = "executorPath"];</code>
       * @return The executorPath.
       */
      public java.lang.String getExecutorPath() {
        java.lang.Object ref = executorPath_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6 [json_name = "executorPath"];</code>
       * @return The bytes for executorPath.
       */
      public com.google.protobuf.ByteString
          getExecutorPathBytes() {
        java.lang.Object ref = executorPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6 [json_name = "executorPath"];</code>
       * @param value The executorPath to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        executorPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6 [json_name = "executorPath"];</code>
       * @return This builder for chaining.
       */
      public Builder clearExecutorPath() {
        
        executorPath_ = getDefaultInstance().getExecutorPath();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6 [json_name = "executorPath"];</code>
       * @param value The bytes for executorPath to set.
       * @return This builder for chaining.
       */
      public Builder setExecutorPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        executorPath_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:flyteidl.plugins.SparkJob)
    }

    // @@protoc_insertion_point(class_scope:flyteidl.plugins.SparkJob)
    private static final com.flyteidl.plugins.SparkProto.SparkJob DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new com.flyteidl.plugins.SparkProto.SparkJob();
    }

    public static com.flyteidl.plugins.SparkProto.SparkJob getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkJob>
        PARSER = new com.google.protobuf.AbstractParser<SparkJob>() {
      @java.lang.Override
      public SparkJob parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e.getMessage()).setUnfinishedMessage(
                  builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SparkJob> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkJob> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public com.flyteidl.plugins.SparkProto.SparkJob getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkApplication_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkJob_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\034flyteidl/plugins/spark.proto\022\020flyteidl" +
      ".plugins\"B\n\020SparkApplication\".\n\004Type\022\n\n\006" +
      "PYTHON\020\000\022\010\n\004JAVA\020\001\022\t\n\005SCALA\020\002\022\005\n\001R\020\003\"\343\003\n" +
      "\010SparkJob\022Q\n\017applicationType\030\001 \001(\0162\'.fly" +
      "teidl.plugins.SparkApplication.TypeR\017app" +
      "licationType\0220\n\023mainApplicationFile\030\002 \001(" +
      "\tR\023mainApplicationFile\022\034\n\tmainClass\030\003 \001(" +
      "\tR\tmainClass\022G\n\tsparkConf\030\004 \003(\0132).flytei" +
      "dl.plugins.SparkJob.SparkConfEntryR\tspar" +
      "kConf\022J\n\nhadoopConf\030\005 \003(\0132*.flyteidl.plu" +
      "gins.SparkJob.HadoopConfEntryR\nhadoopCon" +
      "f\022\"\n\014executorPath\030\006 \001(\tR\014executorPath\032<\n" +
      "\016SparkConfEntry\022\020\n\003key\030\001 \001(\tR\003key\022\024\n\005val" +
      "ue\030\002 \001(\tR\005value:\0028\001\032=\n\017HadoopConfEntry\022\020" +
      "\n\003key\030\001 \001(\tR\003key\022\024\n\005value\030\002 \001(\tR\005value:\002" +
      "8\001B\277\001\n\024com.flyteidl.pluginsB\nSparkProtoH" +
      "\002Z7github.com/flyteorg/flyteidl/gen/pb-g" +
      "o/flyteidl/plugins\370\001\000\242\002\003FPX\252\002\020Flyteidl.P" +
      "lugins\312\002\020Flyteidl\\Plugins\342\002\034Flyteidl\\Plu" +
      "gins\\GPBMetadata\352\002\021Flyteidl::Pluginsb\006pr" +
      "oto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_flyteidl_plugins_SparkApplication_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkApplication_descriptor,
        new java.lang.String[] { });
    internal_static_flyteidl_plugins_SparkJob_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkJob_descriptor,
        new java.lang.String[] { "ApplicationType", "MainApplicationFile", "MainClass", "SparkConf", "HadoopConf", "ExecutorPath", });
    internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor =
      internal_static_flyteidl_plugins_SparkJob_descriptor.getNestedTypes().get(0);
    internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor =
      internal_static_flyteidl_plugins_SparkJob_descriptor.getNestedTypes().get(1);
    internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
