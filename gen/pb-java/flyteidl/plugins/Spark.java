// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: flyteidl/plugins/spark.proto

package flyteidl.plugins;

public final class Spark {
  private Spark() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  public interface SparkApplicationOrBuilder extends
      // @@protoc_insertion_point(interface_extends:flyteidl.plugins.SparkApplication)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code flyteidl.plugins.SparkApplication}
   */
  public  static final class SparkApplication extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:flyteidl.plugins.SparkApplication)
      SparkApplicationOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkApplication.newBuilder() to construct.
    private SparkApplication(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkApplication() {
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkApplication(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkApplication_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              flyteidl.plugins.Spark.SparkApplication.class, flyteidl.plugins.Spark.SparkApplication.Builder.class);
    }

    /**
     * Protobuf enum {@code flyteidl.plugins.SparkApplication.Type}
     */
    public enum Type
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>PYTHON = 0;</code>
       */
      PYTHON(0),
      /**
       * <code>JAVA = 1;</code>
       */
      JAVA(1),
      /**
       * <code>SCALA = 2;</code>
       */
      SCALA(2),
      /**
       * <code>R = 3;</code>
       */
      R(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <code>PYTHON = 0;</code>
       */
      public static final int PYTHON_VALUE = 0;
      /**
       * <code>JAVA = 1;</code>
       */
      public static final int JAVA_VALUE = 1;
      /**
       * <code>SCALA = 2;</code>
       */
      public static final int SCALA_VALUE = 2;
      /**
       * <code>R = 3;</code>
       */
      public static final int R_VALUE = 3;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Type valueOf(int value) {
        return forNumber(value);
      }

      public static Type forNumber(int value) {
        switch (value) {
          case 0: return PYTHON;
          case 1: return JAVA;
          case 2: return SCALA;
          case 3: return R;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Type>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Type> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Type>() {
              public Type findValueByNumber(int number) {
                return Type.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return flyteidl.plugins.Spark.SparkApplication.getDescriptor().getEnumTypes().get(0);
      }

      private static final Type[] VALUES = values();

      public static Type valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Type(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:flyteidl.plugins.SparkApplication.Type)
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof flyteidl.plugins.Spark.SparkApplication)) {
        return super.equals(obj);
      }
      flyteidl.plugins.Spark.SparkApplication other = (flyteidl.plugins.Spark.SparkApplication) obj;

      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static flyteidl.plugins.Spark.SparkApplication parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(flyteidl.plugins.Spark.SparkApplication prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code flyteidl.plugins.SparkApplication}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:flyteidl.plugins.SparkApplication)
        flyteidl.plugins.Spark.SparkApplicationOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkApplication_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                flyteidl.plugins.Spark.SparkApplication.class, flyteidl.plugins.Spark.SparkApplication.Builder.class);
      }

      // Construct using flyteidl.plugins.Spark.SparkApplication.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkApplication_descriptor;
      }

      @java.lang.Override
      public flyteidl.plugins.Spark.SparkApplication getDefaultInstanceForType() {
        return flyteidl.plugins.Spark.SparkApplication.getDefaultInstance();
      }

      @java.lang.Override
      public flyteidl.plugins.Spark.SparkApplication build() {
        flyteidl.plugins.Spark.SparkApplication result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public flyteidl.plugins.Spark.SparkApplication buildPartial() {
        flyteidl.plugins.Spark.SparkApplication result = new flyteidl.plugins.Spark.SparkApplication(this);
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof flyteidl.plugins.Spark.SparkApplication) {
          return mergeFrom((flyteidl.plugins.Spark.SparkApplication)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(flyteidl.plugins.Spark.SparkApplication other) {
        if (other == flyteidl.plugins.Spark.SparkApplication.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        flyteidl.plugins.Spark.SparkApplication parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (flyteidl.plugins.Spark.SparkApplication) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:flyteidl.plugins.SparkApplication)
    }

    // @@protoc_insertion_point(class_scope:flyteidl.plugins.SparkApplication)
    private static final flyteidl.plugins.Spark.SparkApplication DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new flyteidl.plugins.Spark.SparkApplication();
    }

    public static flyteidl.plugins.Spark.SparkApplication getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkApplication>
        PARSER = new com.google.protobuf.AbstractParser<SparkApplication>() {
      @java.lang.Override
      public SparkApplication parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkApplication(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkApplication> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkApplication> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public flyteidl.plugins.Spark.SparkApplication getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkJobOrBuilder extends
      // @@protoc_insertion_point(interface_extends:flyteidl.plugins.SparkJob)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
     */
    int getApplicationTypeValue();
    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
     */
    flyteidl.plugins.Spark.SparkApplication.Type getApplicationType();

    /**
     * <code>string mainApplicationFile = 2;</code>
     */
    java.lang.String getMainApplicationFile();
    /**
     * <code>string mainApplicationFile = 2;</code>
     */
    com.google.protobuf.ByteString
        getMainApplicationFileBytes();

    /**
     * <code>string mainClass = 3;</code>
     */
    java.lang.String getMainClass();
    /**
     * <code>string mainClass = 3;</code>
     */
    com.google.protobuf.ByteString
        getMainClassBytes();

    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */
    int getSparkConfCount();
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */
    boolean containsSparkConf(
        java.lang.String key);
    /**
     * Use {@link #getSparkConfMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getSparkConf();
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getSparkConfMap();
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */

    java.lang.String getSparkConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */

    java.lang.String getSparkConfOrThrow(
        java.lang.String key);

    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */
    int getHadoopConfCount();
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */
    boolean containsHadoopConf(
        java.lang.String key);
    /**
     * Use {@link #getHadoopConfMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getHadoopConf();
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getHadoopConfMap();
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */

    java.lang.String getHadoopConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */

    java.lang.String getHadoopConfOrThrow(
        java.lang.String key);

    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6;</code>
     */
    java.lang.String getExecutorPath();
    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6;</code>
     */
    com.google.protobuf.ByteString
        getExecutorPathBytes();

    /**
     * <pre>
     * Databricks job configuration.
     * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
     * </pre>
     *
     * <code>.google.protobuf.Struct databricksConf = 7;</code>
     */
    boolean hasDatabricksConf();
    /**
     * <pre>
     * Databricks job configuration.
     * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
     * </pre>
     *
     * <code>.google.protobuf.Struct databricksConf = 7;</code>
     */
    com.google.protobuf.Struct getDatabricksConf();
    /**
     * <pre>
     * Databricks job configuration.
     * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
     * </pre>
     *
     * <code>.google.protobuf.Struct databricksConf = 7;</code>
     */
    com.google.protobuf.StructOrBuilder getDatabricksConfOrBuilder();

    /**
     * <pre>
     * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
     * This token can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksToken = 8;</code>
     */
    java.lang.String getDatabricksToken();
    /**
     * <pre>
     * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
     * This token can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksToken = 8;</code>
     */
    com.google.protobuf.ByteString
        getDatabricksTokenBytes();

    /**
     * <pre>
     * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
     * This instance name can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksInstance = 9;</code>
     */
    java.lang.String getDatabricksInstance();
    /**
     * <pre>
     * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
     * This instance name can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksInstance = 9;</code>
     */
    com.google.protobuf.ByteString
        getDatabricksInstanceBytes();
  }
  /**
   * <pre>
   * Custom Proto for Spark Plugin.
   * </pre>
   *
   * Protobuf type {@code flyteidl.plugins.SparkJob}
   */
  public  static final class SparkJob extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:flyteidl.plugins.SparkJob)
      SparkJobOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkJob.newBuilder() to construct.
    private SparkJob(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkJob() {
      applicationType_ = 0;
      mainApplicationFile_ = "";
      mainClass_ = "";
      executorPath_ = "";
      databricksToken_ = "";
      databricksInstance_ = "";
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private SparkJob(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();

              applicationType_ = rawValue;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();

              mainApplicationFile_ = s;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();

              mainClass_ = s;
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                sparkConf_ = com.google.protobuf.MapField.newMapField(
                    SparkConfDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000008;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              sparkConf__ = input.readMessage(
                  SparkConfDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              sparkConf_.getMutableMap().put(
                  sparkConf__.getKey(), sparkConf__.getValue());
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                hadoopConf_ = com.google.protobuf.MapField.newMapField(
                    HadoopConfDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000010;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              hadoopConf__ = input.readMessage(
                  HadoopConfDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              hadoopConf_.getMutableMap().put(
                  hadoopConf__.getKey(), hadoopConf__.getValue());
              break;
            }
            case 50: {
              java.lang.String s = input.readStringRequireUtf8();

              executorPath_ = s;
              break;
            }
            case 58: {
              com.google.protobuf.Struct.Builder subBuilder = null;
              if (databricksConf_ != null) {
                subBuilder = databricksConf_.toBuilder();
              }
              databricksConf_ = input.readMessage(com.google.protobuf.Struct.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(databricksConf_);
                databricksConf_ = subBuilder.buildPartial();
              }

              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();

              databricksToken_ = s;
              break;
            }
            case 74: {
              java.lang.String s = input.readStringRequireUtf8();

              databricksInstance_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 4:
          return internalGetSparkConf();
        case 5:
          return internalGetHadoopConf();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              flyteidl.plugins.Spark.SparkJob.class, flyteidl.plugins.Spark.SparkJob.Builder.class);
    }

    private int bitField0_;
    public static final int APPLICATIONTYPE_FIELD_NUMBER = 1;
    private int applicationType_;
    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
     */
    public int getApplicationTypeValue() {
      return applicationType_;
    }
    /**
     * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
     */
    public flyteidl.plugins.Spark.SparkApplication.Type getApplicationType() {
      @SuppressWarnings("deprecation")
      flyteidl.plugins.Spark.SparkApplication.Type result = flyteidl.plugins.Spark.SparkApplication.Type.valueOf(applicationType_);
      return result == null ? flyteidl.plugins.Spark.SparkApplication.Type.UNRECOGNIZED : result;
    }

    public static final int MAINAPPLICATIONFILE_FIELD_NUMBER = 2;
    private volatile java.lang.Object mainApplicationFile_;
    /**
     * <code>string mainApplicationFile = 2;</code>
     */
    public java.lang.String getMainApplicationFile() {
      java.lang.Object ref = mainApplicationFile_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        mainApplicationFile_ = s;
        return s;
      }
    }
    /**
     * <code>string mainApplicationFile = 2;</code>
     */
    public com.google.protobuf.ByteString
        getMainApplicationFileBytes() {
      java.lang.Object ref = mainApplicationFile_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mainApplicationFile_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int MAINCLASS_FIELD_NUMBER = 3;
    private volatile java.lang.Object mainClass_;
    /**
     * <code>string mainClass = 3;</code>
     */
    public java.lang.String getMainClass() {
      java.lang.Object ref = mainClass_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        mainClass_ = s;
        return s;
      }
    }
    /**
     * <code>string mainClass = 3;</code>
     */
    public com.google.protobuf.ByteString
        getMainClassBytes() {
      java.lang.Object ref = mainClass_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        mainClass_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int SPARKCONF_FIELD_NUMBER = 4;
    private static final class SparkConfDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> sparkConf_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetSparkConf() {
      if (sparkConf_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            SparkConfDefaultEntryHolder.defaultEntry);
      }
      return sparkConf_;
    }

    public int getSparkConfCount() {
      return internalGetSparkConf().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */

    public boolean containsSparkConf(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetSparkConf().getMap().containsKey(key);
    }
    /**
     * Use {@link #getSparkConfMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getSparkConf() {
      return getSparkConfMap();
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */

    public java.util.Map<java.lang.String, java.lang.String> getSparkConfMap() {
      return internalGetSparkConf().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */

    public java.lang.String getSparkConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetSparkConf().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; sparkConf = 4;</code>
     */

    public java.lang.String getSparkConfOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetSparkConf().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int HADOOPCONF_FIELD_NUMBER = 5;
    private static final class HadoopConfDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> hadoopConf_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetHadoopConf() {
      if (hadoopConf_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            HadoopConfDefaultEntryHolder.defaultEntry);
      }
      return hadoopConf_;
    }

    public int getHadoopConfCount() {
      return internalGetHadoopConf().getMap().size();
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */

    public boolean containsHadoopConf(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetHadoopConf().getMap().containsKey(key);
    }
    /**
     * Use {@link #getHadoopConfMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getHadoopConf() {
      return getHadoopConfMap();
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */

    public java.util.Map<java.lang.String, java.lang.String> getHadoopConfMap() {
      return internalGetHadoopConf().getMap();
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */

    public java.lang.String getHadoopConfOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetHadoopConf().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
     */

    public java.lang.String getHadoopConfOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetHadoopConf().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int EXECUTORPATH_FIELD_NUMBER = 6;
    private volatile java.lang.Object executorPath_;
    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6;</code>
     */
    public java.lang.String getExecutorPath() {
      java.lang.Object ref = executorPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        executorPath_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * Executor path for Python jobs.
     * </pre>
     *
     * <code>string executorPath = 6;</code>
     */
    public com.google.protobuf.ByteString
        getExecutorPathBytes() {
      java.lang.Object ref = executorPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        executorPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATABRICKSCONF_FIELD_NUMBER = 7;
    private com.google.protobuf.Struct databricksConf_;
    /**
     * <pre>
     * Databricks job configuration.
     * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
     * </pre>
     *
     * <code>.google.protobuf.Struct databricksConf = 7;</code>
     */
    public boolean hasDatabricksConf() {
      return databricksConf_ != null;
    }
    /**
     * <pre>
     * Databricks job configuration.
     * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
     * </pre>
     *
     * <code>.google.protobuf.Struct databricksConf = 7;</code>
     */
    public com.google.protobuf.Struct getDatabricksConf() {
      return databricksConf_ == null ? com.google.protobuf.Struct.getDefaultInstance() : databricksConf_;
    }
    /**
     * <pre>
     * Databricks job configuration.
     * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
     * </pre>
     *
     * <code>.google.protobuf.Struct databricksConf = 7;</code>
     */
    public com.google.protobuf.StructOrBuilder getDatabricksConfOrBuilder() {
      return getDatabricksConf();
    }

    public static final int DATABRICKSTOKEN_FIELD_NUMBER = 8;
    private volatile java.lang.Object databricksToken_;
    /**
     * <pre>
     * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
     * This token can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksToken = 8;</code>
     */
    public java.lang.String getDatabricksToken() {
      java.lang.Object ref = databricksToken_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        databricksToken_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
     * This token can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksToken = 8;</code>
     */
    public com.google.protobuf.ByteString
        getDatabricksTokenBytes() {
      java.lang.Object ref = databricksToken_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        databricksToken_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATABRICKSINSTANCE_FIELD_NUMBER = 9;
    private volatile java.lang.Object databricksInstance_;
    /**
     * <pre>
     * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
     * This instance name can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksInstance = 9;</code>
     */
    public java.lang.String getDatabricksInstance() {
      java.lang.Object ref = databricksInstance_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        databricksInstance_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
     * This instance name can be set in either flytepropeller or flytekit.
     * </pre>
     *
     * <code>string databricksInstance = 9;</code>
     */
    public com.google.protobuf.ByteString
        getDatabricksInstanceBytes() {
      java.lang.Object ref = databricksInstance_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        databricksInstance_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (applicationType_ != flyteidl.plugins.Spark.SparkApplication.Type.PYTHON.getNumber()) {
        output.writeEnum(1, applicationType_);
      }
      if (!getMainApplicationFileBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, mainApplicationFile_);
      }
      if (!getMainClassBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, mainClass_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetSparkConf(),
          SparkConfDefaultEntryHolder.defaultEntry,
          4);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetHadoopConf(),
          HadoopConfDefaultEntryHolder.defaultEntry,
          5);
      if (!getExecutorPathBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 6, executorPath_);
      }
      if (databricksConf_ != null) {
        output.writeMessage(7, getDatabricksConf());
      }
      if (!getDatabricksTokenBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, databricksToken_);
      }
      if (!getDatabricksInstanceBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 9, databricksInstance_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (applicationType_ != flyteidl.plugins.Spark.SparkApplication.Type.PYTHON.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, applicationType_);
      }
      if (!getMainApplicationFileBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, mainApplicationFile_);
      }
      if (!getMainClassBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, mainClass_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetSparkConf().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        sparkConf__ = SparkConfDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(4, sparkConf__);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetHadoopConf().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        hadoopConf__ = HadoopConfDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(5, hadoopConf__);
      }
      if (!getExecutorPathBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(6, executorPath_);
      }
      if (databricksConf_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, getDatabricksConf());
      }
      if (!getDatabricksTokenBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, databricksToken_);
      }
      if (!getDatabricksInstanceBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(9, databricksInstance_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof flyteidl.plugins.Spark.SparkJob)) {
        return super.equals(obj);
      }
      flyteidl.plugins.Spark.SparkJob other = (flyteidl.plugins.Spark.SparkJob) obj;

      if (applicationType_ != other.applicationType_) return false;
      if (!getMainApplicationFile()
          .equals(other.getMainApplicationFile())) return false;
      if (!getMainClass()
          .equals(other.getMainClass())) return false;
      if (!internalGetSparkConf().equals(
          other.internalGetSparkConf())) return false;
      if (!internalGetHadoopConf().equals(
          other.internalGetHadoopConf())) return false;
      if (!getExecutorPath()
          .equals(other.getExecutorPath())) return false;
      if (hasDatabricksConf() != other.hasDatabricksConf()) return false;
      if (hasDatabricksConf()) {
        if (!getDatabricksConf()
            .equals(other.getDatabricksConf())) return false;
      }
      if (!getDatabricksToken()
          .equals(other.getDatabricksToken())) return false;
      if (!getDatabricksInstance()
          .equals(other.getDatabricksInstance())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + APPLICATIONTYPE_FIELD_NUMBER;
      hash = (53 * hash) + applicationType_;
      hash = (37 * hash) + MAINAPPLICATIONFILE_FIELD_NUMBER;
      hash = (53 * hash) + getMainApplicationFile().hashCode();
      hash = (37 * hash) + MAINCLASS_FIELD_NUMBER;
      hash = (53 * hash) + getMainClass().hashCode();
      if (!internalGetSparkConf().getMap().isEmpty()) {
        hash = (37 * hash) + SPARKCONF_FIELD_NUMBER;
        hash = (53 * hash) + internalGetSparkConf().hashCode();
      }
      if (!internalGetHadoopConf().getMap().isEmpty()) {
        hash = (37 * hash) + HADOOPCONF_FIELD_NUMBER;
        hash = (53 * hash) + internalGetHadoopConf().hashCode();
      }
      hash = (37 * hash) + EXECUTORPATH_FIELD_NUMBER;
      hash = (53 * hash) + getExecutorPath().hashCode();
      if (hasDatabricksConf()) {
        hash = (37 * hash) + DATABRICKSCONF_FIELD_NUMBER;
        hash = (53 * hash) + getDatabricksConf().hashCode();
      }
      hash = (37 * hash) + DATABRICKSTOKEN_FIELD_NUMBER;
      hash = (53 * hash) + getDatabricksToken().hashCode();
      hash = (37 * hash) + DATABRICKSINSTANCE_FIELD_NUMBER;
      hash = (53 * hash) + getDatabricksInstance().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkJob parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static flyteidl.plugins.Spark.SparkJob parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static flyteidl.plugins.Spark.SparkJob parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(flyteidl.plugins.Spark.SparkJob prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Custom Proto for Spark Plugin.
     * </pre>
     *
     * Protobuf type {@code flyteidl.plugins.SparkJob}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:flyteidl.plugins.SparkJob)
        flyteidl.plugins.Spark.SparkJobOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetSparkConf();
          case 5:
            return internalGetHadoopConf();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetMutableSparkConf();
          case 5:
            return internalGetMutableHadoopConf();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                flyteidl.plugins.Spark.SparkJob.class, flyteidl.plugins.Spark.SparkJob.Builder.class);
      }

      // Construct using flyteidl.plugins.Spark.SparkJob.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        applicationType_ = 0;

        mainApplicationFile_ = "";

        mainClass_ = "";

        internalGetMutableSparkConf().clear();
        internalGetMutableHadoopConf().clear();
        executorPath_ = "";

        if (databricksConfBuilder_ == null) {
          databricksConf_ = null;
        } else {
          databricksConf_ = null;
          databricksConfBuilder_ = null;
        }
        databricksToken_ = "";

        databricksInstance_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return flyteidl.plugins.Spark.internal_static_flyteidl_plugins_SparkJob_descriptor;
      }

      @java.lang.Override
      public flyteidl.plugins.Spark.SparkJob getDefaultInstanceForType() {
        return flyteidl.plugins.Spark.SparkJob.getDefaultInstance();
      }

      @java.lang.Override
      public flyteidl.plugins.Spark.SparkJob build() {
        flyteidl.plugins.Spark.SparkJob result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public flyteidl.plugins.Spark.SparkJob buildPartial() {
        flyteidl.plugins.Spark.SparkJob result = new flyteidl.plugins.Spark.SparkJob(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.applicationType_ = applicationType_;
        result.mainApplicationFile_ = mainApplicationFile_;
        result.mainClass_ = mainClass_;
        result.sparkConf_ = internalGetSparkConf();
        result.sparkConf_.makeImmutable();
        result.hadoopConf_ = internalGetHadoopConf();
        result.hadoopConf_.makeImmutable();
        result.executorPath_ = executorPath_;
        if (databricksConfBuilder_ == null) {
          result.databricksConf_ = databricksConf_;
        } else {
          result.databricksConf_ = databricksConfBuilder_.build();
        }
        result.databricksToken_ = databricksToken_;
        result.databricksInstance_ = databricksInstance_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof flyteidl.plugins.Spark.SparkJob) {
          return mergeFrom((flyteidl.plugins.Spark.SparkJob)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(flyteidl.plugins.Spark.SparkJob other) {
        if (other == flyteidl.plugins.Spark.SparkJob.getDefaultInstance()) return this;
        if (other.applicationType_ != 0) {
          setApplicationTypeValue(other.getApplicationTypeValue());
        }
        if (!other.getMainApplicationFile().isEmpty()) {
          mainApplicationFile_ = other.mainApplicationFile_;
          onChanged();
        }
        if (!other.getMainClass().isEmpty()) {
          mainClass_ = other.mainClass_;
          onChanged();
        }
        internalGetMutableSparkConf().mergeFrom(
            other.internalGetSparkConf());
        internalGetMutableHadoopConf().mergeFrom(
            other.internalGetHadoopConf());
        if (!other.getExecutorPath().isEmpty()) {
          executorPath_ = other.executorPath_;
          onChanged();
        }
        if (other.hasDatabricksConf()) {
          mergeDatabricksConf(other.getDatabricksConf());
        }
        if (!other.getDatabricksToken().isEmpty()) {
          databricksToken_ = other.databricksToken_;
          onChanged();
        }
        if (!other.getDatabricksInstance().isEmpty()) {
          databricksInstance_ = other.databricksInstance_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        flyteidl.plugins.Spark.SparkJob parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (flyteidl.plugins.Spark.SparkJob) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int applicationType_ = 0;
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
       */
      public int getApplicationTypeValue() {
        return applicationType_;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
       */
      public Builder setApplicationTypeValue(int value) {
        applicationType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
       */
      public flyteidl.plugins.Spark.SparkApplication.Type getApplicationType() {
        @SuppressWarnings("deprecation")
        flyteidl.plugins.Spark.SparkApplication.Type result = flyteidl.plugins.Spark.SparkApplication.Type.valueOf(applicationType_);
        return result == null ? flyteidl.plugins.Spark.SparkApplication.Type.UNRECOGNIZED : result;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
       */
      public Builder setApplicationType(flyteidl.plugins.Spark.SparkApplication.Type value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        applicationType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <code>.flyteidl.plugins.SparkApplication.Type applicationType = 1;</code>
       */
      public Builder clearApplicationType() {
        
        applicationType_ = 0;
        onChanged();
        return this;
      }

      private java.lang.Object mainApplicationFile_ = "";
      /**
       * <code>string mainApplicationFile = 2;</code>
       */
      public java.lang.String getMainApplicationFile() {
        java.lang.Object ref = mainApplicationFile_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          mainApplicationFile_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string mainApplicationFile = 2;</code>
       */
      public com.google.protobuf.ByteString
          getMainApplicationFileBytes() {
        java.lang.Object ref = mainApplicationFile_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mainApplicationFile_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string mainApplicationFile = 2;</code>
       */
      public Builder setMainApplicationFile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        mainApplicationFile_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string mainApplicationFile = 2;</code>
       */
      public Builder clearMainApplicationFile() {
        
        mainApplicationFile_ = getDefaultInstance().getMainApplicationFile();
        onChanged();
        return this;
      }
      /**
       * <code>string mainApplicationFile = 2;</code>
       */
      public Builder setMainApplicationFileBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        mainApplicationFile_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object mainClass_ = "";
      /**
       * <code>string mainClass = 3;</code>
       */
      public java.lang.String getMainClass() {
        java.lang.Object ref = mainClass_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          mainClass_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>string mainClass = 3;</code>
       */
      public com.google.protobuf.ByteString
          getMainClassBytes() {
        java.lang.Object ref = mainClass_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          mainClass_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string mainClass = 3;</code>
       */
      public Builder setMainClass(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        mainClass_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>string mainClass = 3;</code>
       */
      public Builder clearMainClass() {
        
        mainClass_ = getDefaultInstance().getMainClass();
        onChanged();
        return this;
      }
      /**
       * <code>string mainClass = 3;</code>
       */
      public Builder setMainClassBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        mainClass_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> sparkConf_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetSparkConf() {
        if (sparkConf_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              SparkConfDefaultEntryHolder.defaultEntry);
        }
        return sparkConf_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableSparkConf() {
        onChanged();;
        if (sparkConf_ == null) {
          sparkConf_ = com.google.protobuf.MapField.newMapField(
              SparkConfDefaultEntryHolder.defaultEntry);
        }
        if (!sparkConf_.isMutable()) {
          sparkConf_ = sparkConf_.copy();
        }
        return sparkConf_;
      }

      public int getSparkConfCount() {
        return internalGetSparkConf().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */

      public boolean containsSparkConf(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetSparkConf().getMap().containsKey(key);
      }
      /**
       * Use {@link #getSparkConfMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getSparkConf() {
        return getSparkConfMap();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getSparkConfMap() {
        return internalGetSparkConf().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */

      public java.lang.String getSparkConfOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetSparkConf().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */

      public java.lang.String getSparkConfOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetSparkConf().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearSparkConf() {
        internalGetMutableSparkConf().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */

      public Builder removeSparkConf(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableSparkConf().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableSparkConf() {
        return internalGetMutableSparkConf().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */
      public Builder putSparkConf(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableSparkConf().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; sparkConf = 4;</code>
       */

      public Builder putAllSparkConf(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableSparkConf().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> hadoopConf_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetHadoopConf() {
        if (hadoopConf_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              HadoopConfDefaultEntryHolder.defaultEntry);
        }
        return hadoopConf_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableHadoopConf() {
        onChanged();;
        if (hadoopConf_ == null) {
          hadoopConf_ = com.google.protobuf.MapField.newMapField(
              HadoopConfDefaultEntryHolder.defaultEntry);
        }
        if (!hadoopConf_.isMutable()) {
          hadoopConf_ = hadoopConf_.copy();
        }
        return hadoopConf_;
      }

      public int getHadoopConfCount() {
        return internalGetHadoopConf().getMap().size();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */

      public boolean containsHadoopConf(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetHadoopConf().getMap().containsKey(key);
      }
      /**
       * Use {@link #getHadoopConfMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getHadoopConf() {
        return getHadoopConfMap();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getHadoopConfMap() {
        return internalGetHadoopConf().getMap();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */

      public java.lang.String getHadoopConfOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetHadoopConf().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */

      public java.lang.String getHadoopConfOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetHadoopConf().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearHadoopConf() {
        internalGetMutableHadoopConf().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */

      public Builder removeHadoopConf(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableHadoopConf().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableHadoopConf() {
        return internalGetMutableHadoopConf().getMutableMap();
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */
      public Builder putHadoopConf(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableHadoopConf().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <code>map&lt;string, string&gt; hadoopConf = 5;</code>
       */

      public Builder putAllHadoopConf(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableHadoopConf().getMutableMap()
            .putAll(values);
        return this;
      }

      private java.lang.Object executorPath_ = "";
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6;</code>
       */
      public java.lang.String getExecutorPath() {
        java.lang.Object ref = executorPath_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          executorPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6;</code>
       */
      public com.google.protobuf.ByteString
          getExecutorPathBytes() {
        java.lang.Object ref = executorPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          executorPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6;</code>
       */
      public Builder setExecutorPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        executorPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6;</code>
       */
      public Builder clearExecutorPath() {
        
        executorPath_ = getDefaultInstance().getExecutorPath();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Executor path for Python jobs.
       * </pre>
       *
       * <code>string executorPath = 6;</code>
       */
      public Builder setExecutorPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        executorPath_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.Struct databricksConf_;
      private com.google.protobuf.SingleFieldBuilderV3<
          com.google.protobuf.Struct, com.google.protobuf.Struct.Builder, com.google.protobuf.StructOrBuilder> databricksConfBuilder_;
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public boolean hasDatabricksConf() {
        return databricksConfBuilder_ != null || databricksConf_ != null;
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public com.google.protobuf.Struct getDatabricksConf() {
        if (databricksConfBuilder_ == null) {
          return databricksConf_ == null ? com.google.protobuf.Struct.getDefaultInstance() : databricksConf_;
        } else {
          return databricksConfBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public Builder setDatabricksConf(com.google.protobuf.Struct value) {
        if (databricksConfBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          databricksConf_ = value;
          onChanged();
        } else {
          databricksConfBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public Builder setDatabricksConf(
          com.google.protobuf.Struct.Builder builderForValue) {
        if (databricksConfBuilder_ == null) {
          databricksConf_ = builderForValue.build();
          onChanged();
        } else {
          databricksConfBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public Builder mergeDatabricksConf(com.google.protobuf.Struct value) {
        if (databricksConfBuilder_ == null) {
          if (databricksConf_ != null) {
            databricksConf_ =
              com.google.protobuf.Struct.newBuilder(databricksConf_).mergeFrom(value).buildPartial();
          } else {
            databricksConf_ = value;
          }
          onChanged();
        } else {
          databricksConfBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public Builder clearDatabricksConf() {
        if (databricksConfBuilder_ == null) {
          databricksConf_ = null;
          onChanged();
        } else {
          databricksConf_ = null;
          databricksConfBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public com.google.protobuf.Struct.Builder getDatabricksConfBuilder() {
        
        onChanged();
        return getDatabricksConfFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      public com.google.protobuf.StructOrBuilder getDatabricksConfOrBuilder() {
        if (databricksConfBuilder_ != null) {
          return databricksConfBuilder_.getMessageOrBuilder();
        } else {
          return databricksConf_ == null ?
              com.google.protobuf.Struct.getDefaultInstance() : databricksConf_;
        }
      }
      /**
       * <pre>
       * Databricks job configuration.
       * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
       * </pre>
       *
       * <code>.google.protobuf.Struct databricksConf = 7;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          com.google.protobuf.Struct, com.google.protobuf.Struct.Builder, com.google.protobuf.StructOrBuilder> 
          getDatabricksConfFieldBuilder() {
        if (databricksConfBuilder_ == null) {
          databricksConfBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              com.google.protobuf.Struct, com.google.protobuf.Struct.Builder, com.google.protobuf.StructOrBuilder>(
                  getDatabricksConf(),
                  getParentForChildren(),
                  isClean());
          databricksConf_ = null;
        }
        return databricksConfBuilder_;
      }

      private java.lang.Object databricksToken_ = "";
      /**
       * <pre>
       * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
       * This token can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksToken = 8;</code>
       */
      public java.lang.String getDatabricksToken() {
        java.lang.Object ref = databricksToken_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          databricksToken_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
       * This token can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksToken = 8;</code>
       */
      public com.google.protobuf.ByteString
          getDatabricksTokenBytes() {
        java.lang.Object ref = databricksToken_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          databricksToken_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
       * This token can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksToken = 8;</code>
       */
      public Builder setDatabricksToken(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        databricksToken_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
       * This token can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksToken = 8;</code>
       */
      public Builder clearDatabricksToken() {
        
        databricksToken_ = getDefaultInstance().getDatabricksToken();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
       * This token can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksToken = 8;</code>
       */
      public Builder setDatabricksTokenBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        databricksToken_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object databricksInstance_ = "";
      /**
       * <pre>
       * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
       * This instance name can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksInstance = 9;</code>
       */
      public java.lang.String getDatabricksInstance() {
        java.lang.Object ref = databricksInstance_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          databricksInstance_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
       * This instance name can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksInstance = 9;</code>
       */
      public com.google.protobuf.ByteString
          getDatabricksInstanceBytes() {
        java.lang.Object ref = databricksInstance_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          databricksInstance_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
       * This instance name can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksInstance = 9;</code>
       */
      public Builder setDatabricksInstance(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        databricksInstance_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
       * This instance name can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksInstance = 9;</code>
       */
      public Builder clearDatabricksInstance() {
        
        databricksInstance_ = getDefaultInstance().getDatabricksInstance();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Domain name of your deployment. Use the form &lt;account&gt;.cloud.databricks.com.
       * This instance name can be set in either flytepropeller or flytekit.
       * </pre>
       *
       * <code>string databricksInstance = 9;</code>
       */
      public Builder setDatabricksInstanceBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        databricksInstance_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:flyteidl.plugins.SparkJob)
    }

    // @@protoc_insertion_point(class_scope:flyteidl.plugins.SparkJob)
    private static final flyteidl.plugins.Spark.SparkJob DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new flyteidl.plugins.Spark.SparkJob();
    }

    public static flyteidl.plugins.Spark.SparkJob getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkJob>
        PARSER = new com.google.protobuf.AbstractParser<SparkJob>() {
      @java.lang.Override
      public SparkJob parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SparkJob(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<SparkJob> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<SparkJob> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public flyteidl.plugins.Spark.SparkJob getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkApplication_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkJob_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\034flyteidl/plugins/spark.proto\022\020flyteidl" +
      ".plugins\032\034google/protobuf/struct.proto\"B" +
      "\n\020SparkApplication\".\n\004Type\022\n\n\006PYTHON\020\000\022\010" +
      "\n\004JAVA\020\001\022\t\n\005SCALA\020\002\022\005\n\001R\020\003\"\333\003\n\010SparkJob\022" +
      "@\n\017applicationType\030\001 \001(\0162\'.flyteidl.plug" +
      "ins.SparkApplication.Type\022\033\n\023mainApplica" +
      "tionFile\030\002 \001(\t\022\021\n\tmainClass\030\003 \001(\t\022<\n\tspa" +
      "rkConf\030\004 \003(\0132).flyteidl.plugins.SparkJob" +
      ".SparkConfEntry\022>\n\nhadoopConf\030\005 \003(\0132*.fl" +
      "yteidl.plugins.SparkJob.HadoopConfEntry\022" +
      "\024\n\014executorPath\030\006 \001(\t\022/\n\016databricksConf\030" +
      "\007 \001(\0132\027.google.protobuf.Struct\022\027\n\017databr" +
      "icksToken\030\010 \001(\t\022\032\n\022databricksInstance\030\t " +
      "\001(\t\0320\n\016SparkConfEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005va" +
      "lue\030\002 \001(\t:\0028\001\0321\n\017HadoopConfEntry\022\013\n\003key\030" +
      "\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001B9Z7github.com/f" +
      "lyteorg/flyteidl/gen/pb-go/flyteidl/plug" +
      "insb\006proto3"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new com.google.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
          public com.google.protobuf.ExtensionRegistry assignDescriptors(
              com.google.protobuf.Descriptors.FileDescriptor root) {
            descriptor = root;
            return null;
          }
        };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          com.google.protobuf.StructProto.getDescriptor(),
        }, assigner);
    internal_static_flyteidl_plugins_SparkApplication_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_flyteidl_plugins_SparkApplication_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkApplication_descriptor,
        new java.lang.String[] { });
    internal_static_flyteidl_plugins_SparkJob_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_flyteidl_plugins_SparkJob_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkJob_descriptor,
        new java.lang.String[] { "ApplicationType", "MainApplicationFile", "MainClass", "SparkConf", "HadoopConf", "ExecutorPath", "DatabricksConf", "DatabricksToken", "DatabricksInstance", });
    internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor =
      internal_static_flyteidl_plugins_SparkJob_descriptor.getNestedTypes().get(0);
    internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkJob_SparkConfEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor =
      internal_static_flyteidl_plugins_SparkJob_descriptor.getNestedTypes().get(1);
    internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_flyteidl_plugins_SparkJob_HadoopConfEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    com.google.protobuf.StructProto.getDescriptor();
  }

  // @@protoc_insertion_point(outer_class_scope)
}
