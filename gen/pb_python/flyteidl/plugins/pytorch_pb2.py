# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: flyteidl/plugins/pytorch.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='flyteidl/plugins/pytorch.proto',
  package='flyteidl.plugins',
  syntax='proto3',
  serialized_options=_b('Z7github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/plugins'),
  serialized_pb=_b('\n\x1e\x66lyteidl/plugins/pytorch.proto\x12\x10\x66lyteidl.plugins\"\x7f\n\rElasticConfig\x12\x14\n\x0crdzv_backend\x18\x01 \x01(\t\x12\x14\n\x0cmin_replicas\x18\x02 \x01(\x05\x12\x14\n\x0cmax_replicas\x18\x03 \x01(\x05\x12\x16\n\x0enproc_per_node\x18\x04 \x01(\x05\x12\x14\n\x0cmax_restarts\x18\x05 \x01(\x05\"j\n\x1e\x44istributedPyTorchTrainingTask\x12\x0f\n\x07workers\x18\x01 \x01(\x05\x12\x37\n\x0e\x65lastic_config\x18\x02 \x01(\x0b\x32\x1f.flyteidl.plugins.ElasticConfigB9Z7github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/pluginsb\x06proto3')
)




_ELASTICCONFIG = _descriptor.Descriptor(
  name='ElasticConfig',
  full_name='flyteidl.plugins.ElasticConfig',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='rdzv_backend', full_name='flyteidl.plugins.ElasticConfig.rdzv_backend', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='min_replicas', full_name='flyteidl.plugins.ElasticConfig.min_replicas', index=1,
      number=2, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_replicas', full_name='flyteidl.plugins.ElasticConfig.max_replicas', index=2,
      number=3, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='nproc_per_node', full_name='flyteidl.plugins.ElasticConfig.nproc_per_node', index=3,
      number=4, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_restarts', full_name='flyteidl.plugins.ElasticConfig.max_restarts', index=4,
      number=5, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=52,
  serialized_end=179,
)


_DISTRIBUTEDPYTORCHTRAININGTASK = _descriptor.Descriptor(
  name='DistributedPyTorchTrainingTask',
  full_name='flyteidl.plugins.DistributedPyTorchTrainingTask',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='workers', full_name='flyteidl.plugins.DistributedPyTorchTrainingTask.workers', index=0,
      number=1, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='elastic_config', full_name='flyteidl.plugins.DistributedPyTorchTrainingTask.elastic_config', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=181,
  serialized_end=287,
)

_DISTRIBUTEDPYTORCHTRAININGTASK.fields_by_name['elastic_config'].message_type = _ELASTICCONFIG
DESCRIPTOR.message_types_by_name['ElasticConfig'] = _ELASTICCONFIG
DESCRIPTOR.message_types_by_name['DistributedPyTorchTrainingTask'] = _DISTRIBUTEDPYTORCHTRAININGTASK
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

ElasticConfig = _reflection.GeneratedProtocolMessageType('ElasticConfig', (_message.Message,), dict(
  DESCRIPTOR = _ELASTICCONFIG,
  __module__ = 'flyteidl.plugins.pytorch_pb2'
  # @@protoc_insertion_point(class_scope:flyteidl.plugins.ElasticConfig)
  ))
_sym_db.RegisterMessage(ElasticConfig)

DistributedPyTorchTrainingTask = _reflection.GeneratedProtocolMessageType('DistributedPyTorchTrainingTask', (_message.Message,), dict(
  DESCRIPTOR = _DISTRIBUTEDPYTORCHTRAININGTASK,
  __module__ = 'flyteidl.plugins.pytorch_pb2'
  # @@protoc_insertion_point(class_scope:flyteidl.plugins.DistributedPyTorchTrainingTask)
  ))
_sym_db.RegisterMessage(DistributedPyTorchTrainingTask)


DESCRIPTOR._options = None
# @@protoc_insertion_point(module_scope)
