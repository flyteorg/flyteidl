// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: flyteidl/plugins/pytorch.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_flyteidl_2fplugins_2fpytorch_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_flyteidl_2fplugins_2fpytorch_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3019000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3019004 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_flyteidl_2fplugins_2fpytorch_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_flyteidl_2fplugins_2fpytorch_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[1]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const uint32_t offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_flyteidl_2fplugins_2fpytorch_2eproto;
namespace flyteidl {
namespace plugins {
class DistributedPyTorchTrainingTask;
struct DistributedPyTorchTrainingTaskDefaultTypeInternal;
extern DistributedPyTorchTrainingTaskDefaultTypeInternal _DistributedPyTorchTrainingTask_default_instance_;
}  // namespace plugins
}  // namespace flyteidl
PROTOBUF_NAMESPACE_OPEN
template<> ::flyteidl::plugins::DistributedPyTorchTrainingTask* Arena::CreateMaybeMessage<::flyteidl::plugins::DistributedPyTorchTrainingTask>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace flyteidl {
namespace plugins {

// ===================================================================

class DistributedPyTorchTrainingTask final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:flyteidl.plugins.DistributedPyTorchTrainingTask) */ {
 public:
  inline DistributedPyTorchTrainingTask() : DistributedPyTorchTrainingTask(nullptr) {}
  ~DistributedPyTorchTrainingTask() override;
  explicit constexpr DistributedPyTorchTrainingTask(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  DistributedPyTorchTrainingTask(const DistributedPyTorchTrainingTask& from);
  DistributedPyTorchTrainingTask(DistributedPyTorchTrainingTask&& from) noexcept
    : DistributedPyTorchTrainingTask() {
    *this = ::std::move(from);
  }

  inline DistributedPyTorchTrainingTask& operator=(const DistributedPyTorchTrainingTask& from) {
    CopyFrom(from);
    return *this;
  }
  inline DistributedPyTorchTrainingTask& operator=(DistributedPyTorchTrainingTask&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const DistributedPyTorchTrainingTask& default_instance() {
    return *internal_default_instance();
  }
  static inline const DistributedPyTorchTrainingTask* internal_default_instance() {
    return reinterpret_cast<const DistributedPyTorchTrainingTask*>(
               &_DistributedPyTorchTrainingTask_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(DistributedPyTorchTrainingTask& a, DistributedPyTorchTrainingTask& b) {
    a.Swap(&b);
  }
  inline void Swap(DistributedPyTorchTrainingTask* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(DistributedPyTorchTrainingTask* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  DistributedPyTorchTrainingTask* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<DistributedPyTorchTrainingTask>(arena);
  }
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DistributedPyTorchTrainingTask* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "flyteidl.plugins.DistributedPyTorchTrainingTask";
  }
  protected:
  explicit DistributedPyTorchTrainingTask(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kWorkersFieldNumber = 1,
  };
  // int32 workers = 1 [json_name = "workers"];
  void clear_workers();
  int32_t workers() const;
  void set_workers(int32_t value);
  private:
  int32_t _internal_workers() const;
  void _internal_set_workers(int32_t value);
  public:

  // @@protoc_insertion_point(class_scope:flyteidl.plugins.DistributedPyTorchTrainingTask)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int32_t workers_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_flyteidl_2fplugins_2fpytorch_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// DistributedPyTorchTrainingTask

// int32 workers = 1 [json_name = "workers"];
inline void DistributedPyTorchTrainingTask::clear_workers() {
  workers_ = 0;
}
inline int32_t DistributedPyTorchTrainingTask::_internal_workers() const {
  return workers_;
}
inline int32_t DistributedPyTorchTrainingTask::workers() const {
  // @@protoc_insertion_point(field_get:flyteidl.plugins.DistributedPyTorchTrainingTask.workers)
  return _internal_workers();
}
inline void DistributedPyTorchTrainingTask::_internal_set_workers(int32_t value) {
  
  workers_ = value;
}
inline void DistributedPyTorchTrainingTask::set_workers(int32_t value) {
  _internal_set_workers(value);
  // @@protoc_insertion_point(field_set:flyteidl.plugins.DistributedPyTorchTrainingTask.workers)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__

// @@protoc_insertion_point(namespace_scope)

}  // namespace plugins
}  // namespace flyteidl

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_flyteidl_2fplugins_2fpytorch_2eproto
