// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: flyteidl/plugins/spark.proto

#include "flyteidl/plugins/spark.pb.h"

#include <algorithm>

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

PROTOBUF_PRAGMA_INIT_SEG
namespace flyteidl {
namespace plugins {
constexpr SparkApplication::SparkApplication(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct SparkApplicationDefaultTypeInternal {
  constexpr SparkApplicationDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~SparkApplicationDefaultTypeInternal() {}
  union {
    SparkApplication _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT SparkApplicationDefaultTypeInternal _SparkApplication_default_instance_;
constexpr SparkJob_SparkConfEntry_DoNotUse::SparkJob_SparkConfEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct SparkJob_SparkConfEntry_DoNotUseDefaultTypeInternal {
  constexpr SparkJob_SparkConfEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~SparkJob_SparkConfEntry_DoNotUseDefaultTypeInternal() {}
  union {
    SparkJob_SparkConfEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT SparkJob_SparkConfEntry_DoNotUseDefaultTypeInternal _SparkJob_SparkConfEntry_DoNotUse_default_instance_;
constexpr SparkJob_HadoopConfEntry_DoNotUse::SparkJob_HadoopConfEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct SparkJob_HadoopConfEntry_DoNotUseDefaultTypeInternal {
  constexpr SparkJob_HadoopConfEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~SparkJob_HadoopConfEntry_DoNotUseDefaultTypeInternal() {}
  union {
    SparkJob_HadoopConfEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT SparkJob_HadoopConfEntry_DoNotUseDefaultTypeInternal _SparkJob_HadoopConfEntry_DoNotUse_default_instance_;
constexpr SparkJob::SparkJob(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : sparkconf_(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{})
  , hadoopconf_(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{})
  , mainapplicationfile_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , mainclass_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , executorpath_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , applicationtype_(0)
{}
struct SparkJobDefaultTypeInternal {
  constexpr SparkJobDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~SparkJobDefaultTypeInternal() {}
  union {
    SparkJob _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT SparkJobDefaultTypeInternal _SparkJob_default_instance_;
}  // namespace plugins
}  // namespace flyteidl
static ::PROTOBUF_NAMESPACE_ID::Metadata file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[4];
static const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* file_level_enum_descriptors_flyteidl_2fplugins_2fspark_2eproto[1];
static constexpr ::PROTOBUF_NAMESPACE_ID::ServiceDescriptor const** file_level_service_descriptors_flyteidl_2fplugins_2fspark_2eproto = nullptr;

const uint32_t TableStruct_flyteidl_2fplugins_2fspark_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkApplication, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse, value_),
  0,
  1,
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, applicationtype_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, mainapplicationfile_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, mainclass_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, sparkconf_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, hadoopconf_),
  PROTOBUF_FIELD_OFFSET(::flyteidl::plugins::SparkJob, executorpath_),
};
static const ::PROTOBUF_NAMESPACE_ID::internal::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, -1, sizeof(::flyteidl::plugins::SparkApplication)},
  { 6, 14, -1, sizeof(::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse)},
  { 16, 24, -1, sizeof(::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse)},
  { 26, -1, -1, sizeof(::flyteidl::plugins::SparkJob)},
};

static ::PROTOBUF_NAMESPACE_ID::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::flyteidl::plugins::_SparkApplication_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::flyteidl::plugins::_SparkJob_SparkConfEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::flyteidl::plugins::_SparkJob_HadoopConfEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::flyteidl::plugins::_SparkJob_default_instance_),
};

const char descriptor_table_protodef_flyteidl_2fplugins_2fspark_2eproto[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) =
  "\n\034flyteidl/plugins/spark.proto\022\020flyteidl"
  ".plugins\"B\n\020SparkApplication\".\n\004Type\022\n\n\006"
  "PYTHON\020\000\022\010\n\004JAVA\020\001\022\t\n\005SCALA\020\002\022\005\n\001R\020\003\"\343\003\n"
  "\010SparkJob\022Q\n\017applicationType\030\001 \001(\0162\'.fly"
  "teidl.plugins.SparkApplication.TypeR\017app"
  "licationType\0220\n\023mainApplicationFile\030\002 \001("
  "\tR\023mainApplicationFile\022\034\n\tmainClass\030\003 \001("
  "\tR\tmainClass\022G\n\tsparkConf\030\004 \003(\0132).flytei"
  "dl.plugins.SparkJob.SparkConfEntryR\tspar"
  "kConf\022J\n\nhadoopConf\030\005 \003(\0132*.flyteidl.plu"
  "gins.SparkJob.HadoopConfEntryR\nhadoopCon"
  "f\022\"\n\014executorPath\030\006 \001(\tR\014executorPath\032<\n"
  "\016SparkConfEntry\022\020\n\003key\030\001 \001(\tR\003key\022\024\n\005val"
  "ue\030\002 \001(\tR\005value:\0028\001\032=\n\017HadoopConfEntry\022\020"
  "\n\003key\030\001 \001(\tR\003key\022\024\n\005value\030\002 \001(\tR\005value:\002"
  "8\001B\277\001\n\024com.flyteidl.pluginsB\nSparkProtoH"
  "\002Z7github.com/flyteorg/flyteidl/gen/pb-g"
  "o/flyteidl/plugins\370\001\000\242\002\003FPX\252\002\020Flyteidl.P"
  "lugins\312\002\020Flyteidl\\Plugins\342\002\034Flyteidl\\Plu"
  "gins\\GPBMetadata\352\002\021Flyteidl::Pluginsb\006pr"
  "oto3"
  ;
static ::PROTOBUF_NAMESPACE_ID::internal::once_flag descriptor_table_flyteidl_2fplugins_2fspark_2eproto_once;
const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_flyteidl_2fplugins_2fspark_2eproto = {
  false, false, 804, descriptor_table_protodef_flyteidl_2fplugins_2fspark_2eproto, "flyteidl/plugins/spark.proto", 
  &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_once, nullptr, 0, 4,
  schemas, file_default_instances, TableStruct_flyteidl_2fplugins_2fspark_2eproto::offsets,
  file_level_metadata_flyteidl_2fplugins_2fspark_2eproto, file_level_enum_descriptors_flyteidl_2fplugins_2fspark_2eproto, file_level_service_descriptors_flyteidl_2fplugins_2fspark_2eproto,
};
PROTOBUF_ATTRIBUTE_WEAK const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable* descriptor_table_flyteidl_2fplugins_2fspark_2eproto_getter() {
  return &descriptor_table_flyteidl_2fplugins_2fspark_2eproto;
}

// Force running AddDescriptors() at dynamic initialization time.
PROTOBUF_ATTRIBUTE_INIT_PRIORITY static ::PROTOBUF_NAMESPACE_ID::internal::AddDescriptorsRunner dynamic_init_dummy_flyteidl_2fplugins_2fspark_2eproto(&descriptor_table_flyteidl_2fplugins_2fspark_2eproto);
namespace flyteidl {
namespace plugins {
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* SparkApplication_Type_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_flyteidl_2fplugins_2fspark_2eproto);
  return file_level_enum_descriptors_flyteidl_2fplugins_2fspark_2eproto[0];
}
bool SparkApplication_Type_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))
constexpr SparkApplication_Type SparkApplication::PYTHON;
constexpr SparkApplication_Type SparkApplication::JAVA;
constexpr SparkApplication_Type SparkApplication::SCALA;
constexpr SparkApplication_Type SparkApplication::R;
constexpr SparkApplication_Type SparkApplication::Type_MIN;
constexpr SparkApplication_Type SparkApplication::Type_MAX;
constexpr int SparkApplication::Type_ARRAYSIZE;
#endif  // (__cplusplus < 201703) && (!defined(_MSC_VER) || (_MSC_VER >= 1900 && _MSC_VER < 1912))

// ===================================================================

class SparkApplication::_Internal {
 public:
};

SparkApplication::SparkApplication(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase(arena, is_message_owned) {
  // @@protoc_insertion_point(arena_constructor:flyteidl.plugins.SparkApplication)
}
SparkApplication::SparkApplication(const SparkApplication& from)
  : ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:flyteidl.plugins.SparkApplication)
}



::PROTOBUF_NAMESPACE_ID::Metadata SparkApplication::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_getter, &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_once,
      file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[0]);
}

// ===================================================================

SparkJob_SparkConfEntry_DoNotUse::SparkJob_SparkConfEntry_DoNotUse() {}
SparkJob_SparkConfEntry_DoNotUse::SparkJob_SparkConfEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void SparkJob_SparkConfEntry_DoNotUse::MergeFrom(const SparkJob_SparkConfEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata SparkJob_SparkConfEntry_DoNotUse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_getter, &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_once,
      file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[1]);
}

// ===================================================================

SparkJob_HadoopConfEntry_DoNotUse::SparkJob_HadoopConfEntry_DoNotUse() {}
SparkJob_HadoopConfEntry_DoNotUse::SparkJob_HadoopConfEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void SparkJob_HadoopConfEntry_DoNotUse::MergeFrom(const SparkJob_HadoopConfEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata SparkJob_HadoopConfEntry_DoNotUse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_getter, &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_once,
      file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[2]);
}

// ===================================================================

class SparkJob::_Internal {
 public:
};

SparkJob::SparkJob(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  sparkconf_(arena),
  hadoopconf_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:flyteidl.plugins.SparkJob)
}
SparkJob::SparkJob(const SparkJob& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  sparkconf_.MergeFrom(from.sparkconf_);
  hadoopconf_.MergeFrom(from.hadoopconf_);
  mainapplicationfile_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    mainapplicationfile_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_mainapplicationfile().empty()) {
    mainapplicationfile_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_mainapplicationfile(), 
      GetArenaForAllocation());
  }
  mainclass_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    mainclass_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_mainclass().empty()) {
    mainclass_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_mainclass(), 
      GetArenaForAllocation());
  }
  executorpath_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    executorpath_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_executorpath().empty()) {
    executorpath_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_executorpath(), 
      GetArenaForAllocation());
  }
  applicationtype_ = from.applicationtype_;
  // @@protoc_insertion_point(copy_constructor:flyteidl.plugins.SparkJob)
}

inline void SparkJob::SharedCtor() {
mainapplicationfile_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  mainapplicationfile_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
mainclass_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  mainclass_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
executorpath_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  executorpath_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
applicationtype_ = 0;
}

SparkJob::~SparkJob() {
  // @@protoc_insertion_point(destructor:flyteidl.plugins.SparkJob)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void SparkJob::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  mainapplicationfile_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  mainclass_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  executorpath_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

void SparkJob::ArenaDtor(void* object) {
  SparkJob* _this = reinterpret_cast< SparkJob* >(object);
  (void)_this;
  _this->sparkconf_. ~MapField();
  _this->hadoopconf_. ~MapField();
}
inline void SparkJob::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena) {
  if (arena != nullptr) {
    arena->OwnCustomDestructor(this, &SparkJob::ArenaDtor);
  }
}
void SparkJob::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void SparkJob::InternalSwap(SparkJob* other) {
  using std::swap;
  GetReflection()->Swap(this, other);}

::PROTOBUF_NAMESPACE_ID::Metadata SparkJob::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_getter, &descriptor_table_flyteidl_2fplugins_2fspark_2eproto_once,
      file_level_metadata_flyteidl_2fplugins_2fspark_2eproto[3]);
}

// @@protoc_insertion_point(namespace_scope)
}  // namespace plugins
}  // namespace flyteidl
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkApplication* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkApplication >(Arena* arena) {
  return Arena::CreateMessageInternal< ::flyteidl::plugins::SparkApplication >(arena);
}
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::flyteidl::plugins::SparkJob_SparkConfEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::flyteidl::plugins::SparkJob_HadoopConfEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::flyteidl::plugins::SparkJob* Arena::CreateMaybeMessage< ::flyteidl::plugins::SparkJob >(Arena* arena) {
  return Arena::CreateMessageInternal< ::flyteidl::plugins::SparkJob >(arena);
}
PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
